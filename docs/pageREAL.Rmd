---
title: |
  | Central Valley Enhanced
  | Acoustic Tagging Project
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)

```

```{r logos, echo=FALSE, cache=TRUE}
htmltools::img(src = knitr::image_uri("../data/logos.jpg"), 
               alt = 'logo', 
               style = 'position:absolute; top:10px; right:0px; width:200px;')
```

<br/>


# *Real-time tracking Diagnostics*


```{r bring in data, massage, create new data files, print diagnostics, echo=FALSE, warning=FALSE, message=FALSE, results = "hide"}

library(knitr)
library(dplyr)
library(data.table)

dirs <- list.dirs(path= "C:/Advanced Telemetry Systems, Inc/ATS Trident Receiver/Data")
setwd(paste(file.path(Sys.getenv("USERPROFILE"),"Desktop",fsep="\\"), "\\Real-time data massaging\\products", sep = ""))


ATS_processed_files <- read.csv("ATS_processed_files.csv", stringsAsFactors = F)
TECKNO_processed_files <- read.csv("TECKNO_processed_files.csv", stringsAsFactors = F)
gen_locs <- read.csv("realtime_locs.csv", stringsAsFactors = F)
gen_locs$recv <- as.character(gen_locs$recv)
downloads <- read.csv("downloads.csv", stringsAsFactors = FALSE)
downloads$end <- as.POSIXct(downloads$end, format = "%Y-%m-%d %H:%M:%S", tz = "Etc/GMT+8")
downloads$start <- as.POSIXct(downloads$start, format = "%Y-%m-%d %H:%M:%S", tz = "Etc/GMT+8")
## remove all gen loc details from downloads, that way these get re-assigned every hour in case gen_loc data changes. This gets reassigned at the end of this code before exporting
downloads <- downloads[,-which(names(downloads) %in% c("location", "general_location", "latitude", "longitude", "rkm"))]

corrupted_downloads <- read.csv("corrupted_downloads.csv", stringsAsFactors = F)
tagcodes <- read.csv("qry_HexCodes.txt", stringsAsFactors = F)
tagcodes$RelDT <- as.POSIXct(tagcodes$RelDT, format = "%m/%d/%Y %I:%M:%S %p", tz = "Etc/GMT+8")

detects <- fread("detects_final.csv", stringsAsFactors = FALSE)
detects_beacon <- fread("detects_beacon.csv", stringsAsFactors = FALSE)

detects$DateTime_PST <- as.POSIXct(detects$DateTime_PST, format = "%Y-%m-%d %H:%M:%OS ", tz = "Etc/GMT+8")
detects_beacon$DateTime_PST <- as.POSIXct(detects_beacon$DateTime_PST, format = "%Y-%m-%d %H:%M:%OS ", tz = "Etc/GMT+8")

## Now remove all gen loc data from detections in case genloc details have changed. We will add genloc data back in at the end of this code
detects <- subset(detects, select= -c(location, general_location, latitude, longitude, rkm))
detects_beacon <- subset(detects_beacon, select= -c(location, general_location, latitude, longitude, rkm))

## Find all Teckno file names
TECKNO_all_files <- as.data.frame(dir(pattern = "*.txt", path = "C:/Users/Trailer/Desktop/data", full.names = T))
colnames(TECKNO_all_files) <- "x"
TECKNO_all_files$x <- as.character(TECKNO_all_files$x)
## Select only newest TECKNO files for massaging
TECKNO_files <- as.data.frame(TECKNO_all_files[!TECKNO_all_files$x %in% TECKNO_processed_files$x,])
colnames(TECKNO_files) <- "x"
TECKNO_files$x <- as.character(TECKNO_files$x)

## Find all ATS file names
ATS_all_files <- as.data.frame(dir(pattern = "*.csv", path = dirs, full.names = T))
colnames(ATS_all_files) <- "x"
ATS_all_files$x <- as.character(ATS_all_files$x)
## FOR NOW, REMOVE ALL FILES FROM RECEIVER 17135 SINCE IT PRODUCES SO MANY FALSE DETECTIONS
ATS_all_files <- as.data.frame(ATS_all_files[!ATS_all_files$x %like% "SR17135",])
colnames(ATS_all_files) <- "x"
ATS_all_files$x <- as.character(ATS_all_files$x)
## Select only newest ATS files for massaging
ATS_files <- as.data.frame(ATS_all_files[!ATS_all_files$x %in% ATS_processed_files$x,])
#ATS_files <- data.frame()
colnames(ATS_files) <- "x"
ATS_files$x <- as.character(ATS_files$x)
## Remove bad files that don't contain real data from list for processing
bad_files <- read.csv("bad_files.csv", colClasses = "character", header = F)
ATS_files <- as.data.frame(ATS_files[-grep(paste(bad_files$V1, collapse = "|"), ATS_files$x),])
colnames(ATS_files) <- "x"
ATS_files$x <- as.character(ATS_files$x)
## Load CSt timezone file list
CST_files <- read.csv("receiver_files_with_CST_timezones.csv", colClasses = "character")

#### Data massage ATS files first ####

downloads_new_all <- data.frame()

if (nrow(ATS_files) > 0) {
  
  ## First, find shortname for ATS files
  shortname <- regexpr(ATS_files$x, pattern = "Data")[1]
  incomplete_shortname <- substr(ATS_files$x, shortname+5, nchar(ATS_files$x))
  shortname <- strsplit(incomplete_shortname, "/")
  ATS_files_shortname <- unlist(lapply(shortname, tail, 1))

  downloads_new <- data.frame()
  
  for (i in 1:nrow(ATS_files)) {
    
    if (substr(read.csv(ATS_files[i,], nrows = 1, header = F)[,1] , 1,4) == "Site") {
      hour <- read.csv(ATS_files[i,], stringsAsFactors = F, skip = 7, nrows = 1, header = F)
      recv <- read.csv(ATS_files[i,], stringsAsFactors = F, skip = 2, nrows = 1, header = F)
    }else{
      recv <- read.csv(ATS_files[i,], stringsAsFactors = F, skip = 6, nrows = 1, header = F)
      hour <- read.csv(ATS_files[i,], stringsAsFactors = F, skip = 19, nrows = 1, header = F)
    }
    hour <- substr(hour, start = 13, stop = 31)
    recv <- substr(recv, start = 16, stop = 21)
    downloads_new <- rbind(downloads_new, cbind(filename = ATS_files_shortname[i],recv, hour))
  }
  
  downloads_new$start <- as.POSIXct(downloads_new$hour, format = "%m/%d/%Y %H:%M:%S", tz = "Etc/GMT+8")
  
  ## Now change downloads start and end to correct time since some are on CST and some are PST + 1 day
  
  for (i in 1:nrow(downloads_new)){ 
    if(downloads_new[i, "filename"] %in% CST_files$Filenames | 
       downloads_new[i, "recv"] %in% c("17131","18007")) {
      downloads_new[i, "start"] <- downloads_new[i, "start"]-2*60*60
    }
    if(downloads_new[i,"recv"] %in% c("17138", "17125", "17129", "18008", "17143", "17135")) {
      downloads_new[i, "start"] <- downloads_new[i, "start"]-24*60*60
    }
  }
  
  downloads_new$end <- downloads_new$start +(60*60)
  
  ## Add dummy columns to match the ones from Tecknos that actually carry data
  downloads_new$sun_volts <- NA
  downloads_new$bat_volts <- NA
  
  detects_temp <- data.frame()
  
  for (i in 1:nrow(ATS_files)) {
    if (substr(read.csv(ATS_files[i,], nrows = 1, header = F)[,1] , 1,4) == "Site") {
      test <- read.csv(ATS_files[i,], stringsAsFactors = F, skip = 9, header = T,  row.names=NULL)
    }else{
      test <- read.csv(ATS_files[i,], stringsAsFactors = F, skip = 22, header = T,  row.names=NULL)
    } 
    if(colnames(test)[1] != "Internal"){
      colnames(test) <- colnames(test)[-1]
      test[,ncol(test)] <- NULL
    }
    test$recv <- substr(ATS_files[i,], nchar(ATS_files[i,])-22, nchar(ATS_files[i,])-18)
    end1 <- head(grep("--", test[,1]),1)
    end2 <- head(grep("File End", test[,1]),1)
    end_final <- head(grep(".-- Done", test[,1]),1)
    
    if (length(end_final) == 0) {
      ## This means the detection file is incomplete aka corrupted
      ## If the file is corrupted and new, add to corrupted list
      ## if the file is corrupted and a repeat, tally 1 to the counter
      ## At the end of this code, Remove the file from the downloads table
      ## And physically remove the file from the computer if the tally counter is <3
      ## If >= 3, leave the file as this will avoid having to download it again to no avail
      ## We do this at the end so we don't delete files when prototyping this ATS code segment
      downloads_new <- downloads_new[!downloads_new$filename == ATS_files_shortname[i],]
      if(ATS_files_shortname[i] %in% corrupted_downloads$V1){
        corrupted_downloads[corrupted_downloads$V1 == ATS_files_shortname[i], "download_attempts"] <- 
          as.numeric(corrupted_downloads[corrupted_downloads$V1 == ATS_files_shortname[i], "download_attempts"])+1
      }else{
        corrupted_downloads <- rbind(corrupted_downloads, cbind(V1 = ATS_files_shortname[i], download_attempts = 1, filename = ATS_files[i,]))
      }
      
    }else{
      
      ## if a previously corrupted file did finally get read correctly, remove it from the corrupted file
      if(ATS_files_shortname[i] %in% corrupted_downloads$V1){
        corrupted_downloads <- corrupted_downloads[!corrupted_downloads$V1 == ATS_files_shortname[i],] 
      }
      ## Now data massage  
      test1 <- test[-c(end_final:nrow(test)),]
      test2 <- test1[!test1$TagCode == "",]
      test3 <- test2[!test2$TagCode == " TagCode",]
      
      test4 <- test3[!test3$TagCode == " GPS Fix  ",]
      
      names(test4)[names(test4) == 'DateTime'] <- 'DateTime_Orig'
      
      test4$DateTime_Orig <- as.POSIXct(test4$DateTime_Orig, format = "%m/%d/%Y %H:%M:%OS", tz = "Etc/GMT+8")
      
      test4$DateTime_PST <- test4$DateTime_Orig
      
      ## Now change DateTime_PST to PST, but this is tricky because for some, orig time is in CST
      ## Have this first if statement in case there are no detections in a file
      if(nrow(test4)>0){
        if(ATS_files[i,] %in% CST_files$Filenames | 
          unique(test4$recv) %in% c("17131","18007")) {
          test4$DateTime_PST <- test4$DateTime_PST-2*60*60
          attributes(test4$DateTime_PST)$tzone <- "Etc/GMT+8"
        }
        if(unique(test4$recv) %in% c("17138","17125", "17129", "18008", "17143", "17135")) {
          test4$DateTime_PST <- test4$DateTime_PST-24*60*60
          attributes(test4$DateTime_PST)$tzone <- "Etc/GMT+8"
        }
        
        ## change Datetime_Orig with milliseconds shown (this turns the class into character)
        test4$DateTime_Orig <- format(test4$DateTime_Orig, "%Y-%m-%d %H:%M:%OS6")
        
        detects_temp <- rbind(detects_temp, test4)
        
      }
    }
  }
  
  downloads_new_all <- rbind(downloads_new_all, downloads_new[,c("filename", "recv", "hour", "sun_volts", "bat_volts", "end", "start")])
  
  #locs <- detects_new[detects_new$TagCode == " GPS Fix  ",]
  #N <- unlist(gregexpr(pattern ='N',locs$Internal))
  #locs$lat <- as.numeric(substr(locs$Internal, start = 1, stop = N-1))
  #locs$lon <- as.numeric(substr(locs$Internal, start = N+1, stop= nchar(locs$Internal)-2))
  #locs$lat <- round(locs$lat, 0)/100
  #locs$lon <- round(locs$lon, 0)/100*-1
  
  #coords <- unique(locs[,c("recv", "lat", "lon")])
  
  ## In case all new files were corrupted, have this if statement
  if (length(detects_temp) > 0){
   ## Now remove prefix and suffix of TagCode
    detects_temp$TagCode <- substr(detects_temp$TagCode, start = 5, stop = 8)
    
    ## Remove "ffff" detections
    detects_temp <- detects_temp[!detects_temp$TagCode == "ffff",]
    
    ## Here, put only non-beacon detections in detection file
    detects_new <- detects_temp[!detects_temp$TagCode %in% c(gen_locs$beacon, "006F", "006D"),]
    ## And put beacon detections into seperate file
    detects_beacon_new <- detects_temp[detects_temp$TagCode %in% c(gen_locs$beacon, "006F", "006D"),]
    
    #detects_new <- merge(detects_new, coords, by = "recv", all.x = T)
    
    #detects_new <- merge(detects_new, gen_locs, by = "recv", all.x = T)
    
    detects_beacon <- rbind(detects_beacon, detects_beacon_new[,c("recv", "DateTime_Orig", "TagCode", "DateTime_PST", "Temp")])
    detects <- rbind(detects, detects_new[,c("recv", "DateTime_Orig", "TagCode", "DateTime_PST", "Temp")])
  }  
}





#######################################
#### Now data massage TECKNO files ####
#######################################




if (nrow(TECKNO_files) > 0) {
  downloads_new <- data.frame()
  for (i in 1:nrow(TECKNO_files)) {
    test <- try(read.csv(TECKNO_files[i,], header = F, stringsAsFactors = F ), silent = T) 
    if (inherits(test, "try-error") | ncol(test) < 18) {
      next
    } else {
      test1 <- tail(test[nchar(test$V5) > 4,],1)
      hour <- test1$V3
      recv <- test1$V1
      ## Get volts, sometimes in row 1 or 2
      sun_volts <- max(test[1:2,"V17"])
      bat_volts <- max(test[1:2,"V18"])
      downloads_new <- rbind(downloads_new, cbind(filename = TECKNO_files[i,], recv, hour, sun_volts, bat_volts))
    }
  }
  
  downloads_new$end <- as.POSIXct(downloads_new$hour, tz = "GMT")
  downloads_new$start <- downloads_new$end -(60*60)

  
  
  ## Now change all times to PST to match ATS receivers
  attributes(downloads_new$start)$tzone <- "Etc/GMT+8"
  attributes(downloads_new$end)$tzone <- "Etc/GMT+8"
  
  downloads_new_all <- rbind(downloads_new_all, downloads_new)
  
  detects_temp <- data.frame()
  
  ## Put in progress bar
  #pb <- txtProgressBar(min = 0, max = nrow(TECKNO_files), style = 3)
  
  for (i in 1:nrow(TECKNO_files)) {
    res <- try(read.csv(TECKNO_files[i,], header = F), silent = T) 
    if (inherits(res, "try-error")) {
      corrupted <- strsplit(TECKNO_files[i,], "/")
      corrupted2 <- unlist(lapply(corrupted, tail, 1))
      corrupted_downloads <- rbind(corrupted_downloads,cbind(V1 = corrupted2, download_attempts = 3, filename = TECKNO_files[i,]))
      ## Also make corrupted downloads more palateable
      next
    } else {
      start <- nchar(read.csv(TECKNO_files[i,],stringsAsFactors = F, header = F)$V5)
      start <- head(which(start==4),1)
      if (length(start) > 0) {
        test <- read.csv(TECKNO_files[i,], skip = start-1, header = F, stringsAsFactors = F )
        test1 <- test[nchar(test$V5)==4,]
        detects_temp <- rbind(detects_temp, test1[,1:9])
        
      }    
    }
    #setTxtProgressBar(pb, i)
  }
  #close(pb)
  
  colnames(detects_temp) <- c("recv", "V2", "DateTime_Orig", "dec_seconds", "TagCode", "V6", "V7", "V8", "V9")
  detects_temp <- detects_temp[!detects_temp$V2 == "N",]
  ## In this step, append datetime to milliseconds, but make sure the 6 decimal places are represented for all cases, even when there are trailering zeros
  detects_temp$DateTime_Orig <- paste(detects_temp$DateTime_Orig, substr(formatC(as.numeric(detects_temp$dec_seconds),format='f', digits=6 ),2,8), sep = "")
  ## Make a POSIX class, note that TECKNO detections are in GMT, will need to convert to local time
  detects_temp$DateTime_PST <- as.POSIXct(detects_temp$DateTime_Orig, format = "%Y-%m-%d %H:%M:%OS", tz = "GMT")
  attributes(detects_temp$DateTime_PST)$tzone <- "Etc/GMT+8"
  
  ## Add column for temperature
  detects_temp$Temp <- NA
  
  ## Here, put only non-beacon detections in detection file
  detects_new <- detects_temp[!detects_temp$TagCode %in% c(gen_locs$beacon, "006F", "006D"),]
  ## And put beacon detections into seperate file
  detects_beacon_new <- detects_temp[detects_temp$TagCode %in% c(gen_locs$beacon, "006F", "006D"),]
  
  ## Now append these to existing datasets
  detects <- rbind(detects, detects_new[,c("recv", "DateTime_Orig", "TagCode", "DateTime_PST", "Temp")])
  detects_beacon <- rbind(detects_beacon, detects_beacon_new[,c("recv", "DateTime_Orig", "TagCode", "DateTime_PST", "Temp")])
  
}

## Now merge location data back with detections
detects <- merge(detects, gen_locs[,c("recv", "location", "general_location", "latitude", "longitude", "rkm")],  by = "recv", all.x = TRUE)
detects_beacon <- merge(detects_beacon, gen_locs[,c("recv", "location", "general_location", "latitude", "longitude", "rkm")],  by = "recv", all.x = TRUE)

## Merge new downloads with exist dataset
downloads <- rbind(downloads, downloads_new_all)

## Merge in gen loc data now for all downloads
downloads <-  merge(downloads, gen_locs[,c("recv", "location", "general_location", "latitude", "longitude", "rkm")],  by = "recv", all.x = TRUE)
#downloads_new <-  merge(downloads_new, gen_locs[,c("recv", "location", "general_location", "latitude", "longitude", "rkm")],  by = "recv")

## Check for hourly download gaps ##
downloads$gapstart <- as.POSIXct(NA)
downloads$gapend <- as.POSIXct(NA)

downloads <- downloads[order(downloads$recv, downloads$start),]

for (j in 2:nrow(downloads)){
  if(downloads[j-1, "recv"] == downloads[j, "recv"] & difftime(downloads[j, "start"], downloads[j-1, "start"], units = "hours") > 1.25 )
  {downloads[j-1,"gapstart"] <- downloads[j-1, "end"]
  downloads[j-1,"gapend"] <- downloads[j, "start"]
  }
  if (downloads[j-1, "recv"] != downloads[j, "recv"] & difftime(Sys.time(), downloads[j-1, "start"], units = "hours") > 2.2) {
    downloads[j-1,"gapstart"] <- downloads[j-1, "end"]
    downloads[j-1,"gapend"] <- Sys.time()
  }
}

gaps <- downloads[is.na(downloads$gapstart) == F,]
gaps$gapstart <- as.POSIXct(gaps$gapstart,format = "%Y-%m-%d %H:%M:%S", tz = "Etc/GMT+8")
gaps$gapend <- as.POSIXct(gaps$gapend, format = "%Y-%m-%d %H:%M:%S", tz = "Etc/GMT+8")
 
gaps$gap_length_hours <- round(difftime(gaps$gapend, gaps$gapstart, units = "hours"), 0)
gaps$start <- NULL
gaps$end <- NULL

gap_percent <- aggregate(list(total_files = downloads$start), by = list(recv = downloads$recv), FUN = length) 
gap_percent <- merge(gap_percent,aggregate(list(recv_start = downloads$start), by = list(recv = downloads$recv), FUN = min), by = "recv", all.y = T)
## Some receivers turned hourly recording off soon after deployment, meaning there is an initial gap in the data, so I'm setting the true start time to when it started uninterrupted recording (i.e. after the gap)
gap_percent[gap_percent$recv == "17135", "recv_start"] <- as.POSIXct("2018-05-14 10:00:00")
gap_percent[gap_percent$recv == "18007", "recv_start"] <- as.POSIXct("2018-04-18 15:00:00")
gap_percent[gap_percent$recv == "17140", "recv_start"] <- as.POSIXct("2018-04-17 9:00:00")
gap_percent[gap_percent$recv == "17142", "recv_start"] <- as.POSIXct("2018-04-17 4:00:00")
gap_percent[gap_percent$recv == "17125", "recv_start"] <- as.POSIXct("2018-04-24 18:00:00")
gap_percent[gap_percent$recv == "18008", "recv_start"] <- as.POSIXct("2018-04-25 17:00:00")
gap_percent[gap_percent$recv == "17130", "recv_start"] <- as.POSIXct("2018-05-16 12:00:00")

gap_percent$num_of_hours <- round(difftime(Sys.time(), gap_percent$recv_start, units = "h"), 0)
gap_percent$percent_files_missing <- 100-round(gap_percent$total_files/as.numeric(gap_percent$num_of_hours)*100,1)

downloads$gapstart <- NULL
downloads$gapend <- NULL



## Find data gaps by finding hours for which there are zero detectsion
## These should never happen since there are so many beacon detections

# res <- aggregate(detects_beacon$TagCode,
#                  by=list(detects_beacon$recv, format(detects_beacon$DateTime_PST, "%Y-%m-%d %H")),
#                  length)
# 
# colnames(res) <- c("recv", "DateTimeHour_PST", "detect_count")
# 
# hours <- as.data.frame(format(seq(
#   from= min(detects_beacon$DateTime_PST)+60*60*24,
#   to=as.POSIXct(Sys.time(), tz="Etc/GMT+8")-60*60*12,
#   by="hour"
# ),
# "%Y-%m-%d %H"))
# 
# colnames(hours) <- "DateTimeHour_PST"
# 
# res2 <- dcast(data = res, DateTimeHour_PST ~ recv)
# 
# res3 <- merge(hours, res2, by = "DateTimeHour_PST", all.x = T)
# 
# gaps <- res3[rowSums(is.na(res3)) > 0,]
# 
# gaps <- melt(gaps)
# 
# gaps <- gaps[is.na(gaps$value)==T,]
# 
# gaps$value <- NULL
# 
# colnames(gaps) <- c("DateHour_PST", "recv")


#### As a preliminary cut to detection filter, a tag needs to have been detected at least 3 times in the entire array


tags <- sort(table(detects$TagCode), decreasing = T)
tags <- tags[tags>2]
detects_filtered <- detects[detects$TagCode %in% rownames(tags),]
detects_filtered <- detects_filtered[order(detects_filtered$general_location, detects_filtered$TagCode, detects_filtered$DateTime_PST),]

#### Now, subset for only tags with 2 or more detects per gen loc within 24 hours that aren't multipath, by studyID

for (k in unique(tagcodes$StudyID)){
  studycodes <- tagcodes[tagcodes$StudyID == k, "TagID_Hex"]
  ## First, select only detections of correct tagcodes per study
  temp_detects <- as.data.frame(detects_filtered[detects_filtered$TagCode %in% studycodes,])
  ## Now remove false detections that occured before release
  temp_detects <- merge(temp_detects, tagcodes[,c("TagID_Hex", "RelDT")], by.x = "TagCode", by.y = "TagID_Hex")
  temp_detects <- temp_detects[temp_detects$DateTime_PST > temp_detects$RelDT,]
  ## Now if a tag was detected less than 3 times at any gen loc, remove those detections at that gen loc
  detect_count <- table(temp_detects$general_location, temp_detects$TagCode)
  detect_count_melt <- melt(detect_count)
  detect_count_melt <- detect_count_melt[detect_count_melt$value >2,]
  detect_count_melt$genloctag <- paste(detect_count_melt$Var1, detect_count_melt$Var2, sep = "-")
  temp_detects$genloctag <- paste(temp_detects$general_location, temp_detects$TagCode, sep = "-")
  temp_detects <- temp_detects[temp_detects$genloctag %in% detect_count_melt$genloctag,]
  
  if(nrow(temp_detects) > 0){
    ## Now create two columns with time in hours between the previous and next detection, for each detection
        ## Now reorder the data since the above steps may have jumbled things
    temp_detects <- temp_detects[order(temp_detects$general_location, temp_detects$TagCode, temp_detects$DateTime_PST),]
    temp_detects$time_to_next <- as.numeric(difftime(shift(temp_detects$DateTime_PST, fill = NA, type = "lead"), temp_detects$DateTime_PST, units = "hours"))
    temp_detects$time_from_previous <- as.numeric(difftime(temp_detects$DateTime_PST, shift(temp_detects$DateTime_PST, fill = NA, type = "lag"),  units = "hours"))
    ## Now make NA the time diff values when it's between 2 different gen locs or tagcodes
    temp_detects[which(temp_detects$general_location != shift(temp_detects$general_location, fill = NA, type = "lag")), "time_from_previous"] <- NA
    temp_detects[which(temp_detects$general_location != shift(temp_detects$general_location, fill = NA, type = "lead")), "time_to_next"] <- NA
    temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
    temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, fill = NA, type = "lead")), "time_to_next"] <- NA
    ## Now remove single detections per GenLoc, by removing records with NA in the lead and lag columns
    temp_detects <- temp_detects[!with(temp_detects, is.na(temp_detects$time_to_next)& is.na(temp_detects$time_from_previous)),]
    ## Now remove the second detection every time the detection lag difftime is less than 1 sec (multipath)
    temp_detects <- temp_detects[temp_detects$time_from_previous > 0.00028 | is.na(temp_detects$time_from_previous) == TRUE ,]
    ## Now re-estimate the time in hours between the previous and next detection, for each detection. just repeating those earlier lines of code
    temp_detects$time_to_next <- as.numeric(difftime(shift(temp_detects$DateTime_PST, fill = NA, type = "lead"), temp_detects$DateTime_PST, units = "hours"))
    temp_detects$time_from_previous <- as.numeric(difftime(temp_detects$DateTime_PST, shift(temp_detects$DateTime_PST, fill = NA, type = "lag"),  units = "hours"))
    ## Now make NA the time diff values when it's between 2 different gen locs or tagcodes
    temp_detects[which(temp_detects$general_location != shift(temp_detects$general_location, fill = NA, type = "lag")), "time_from_previous"] <- NA
    temp_detects[which(temp_detects$general_location != shift(temp_detects$general_location, fill = NA, type = "lead")), "time_to_next"] <- NA
    temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
    temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, fill = NA, type = "lead")), "time_to_next"] <- NA

    ## Now remove RelDT and genloctag columns
    temp_detects <- temp_detects[, !names(temp_detects) %in% c("RelDT", "genloctag")]
    ## Now pull out only tag detections that have another detection from the same tag at the same gen loc within 24 hours
    temp_detects <- temp_detects[which(apply(temp_detects[,c("time_to_next", "time_from_previous")],1,FUN=min, na.rm = T) < 24),]
  }
  write.csv(temp_detects, paste(paste("detects",k,sep = "_"),".csv", sep = ""), row.names = F)
}


latest <- as.character(round.POSIXt(max(downloads$end), units = "hour"))

## Now that all data manipulations are complete, export detects with milliseconds shown (this turns the class into character)
detects$DateTime_PST <- format(detects$DateTime_PST, "%Y-%m-%d %H:%M:%OS6")
detects_beacon$DateTime_PST <- format(detects_beacon$DateTime_PST, "%Y-%m-%d %H:%M:%OS6")

write.csv(latest, "latest_download.csv", row.names = F)
write.csv(downloads, "downloads.csv", row.names = F)
fwrite(detects, "detects_final.csv", row.names = F)
fwrite(detects_beacon, "detects_beacon.csv", row.names = F)
#write.csv(detects_filtered, "detects_filtered.csv", row.names = F)
write.csv(gaps, "download_gaps.csv", row.names = F)

## Now delete the corrupted files that have been redownloaded <3 times, with hopes that they will be better when redownloaded, and remove them from ATS_processed_files so script knows to reprocess them
remove_these <- corrupted_downloads[corrupted_downloads$download_attempts <3,"filename"]
file.remove(remove_these)

write.csv(corrupted_downloads, "corrupted_downloads.csv", row.names = F)

ATS_processed_files <- rbind(ATS_processed_files, ATS_files)

ATS_processed_files <- as.data.frame(ATS_processed_files[!ATS_processed_files$x %in% remove_these,])
colnames(ATS_processed_files) <- "x"
write.csv(ATS_processed_files, "ATS_processed_files.csv", row.names = F)
TECKNO_processed_files <- rbind(TECKNO_processed_files, TECKNO_files)
write.csv(TECKNO_processed_files, "TECKNO_processed_files.csv", row.names = F)  




```
*Data current as of `r latest`*

<br/>

## Receiver sites
``` {r print site stats, echo=FALSE, warning=FALSE}

last_download <- aggregate(list(last_download = downloads$end), by = list(location = downloads$location), FUN = max)

gen_locs <- merge(gen_locs[,c("recv", "location", "general_location", "latitude", "longitude", "rkm")], last_download, by = "location", all.x = T)

gen_locs <- gen_locs[order(gen_locs$last_download, decreasing = T),]

gen_locs <- merge(gen_locs, gap_percent[c("recv", "recv_start", "percent_files_missing")], by = "recv", all.x = T)

kable(gen_locs[,c("recv", "location", "general_location", "latitude", "longitude", "rkm",  "recv_start", "last_download", "percent_files_missing")], align = "c")


````

<br/>

## Most recent downloads
``` {r print most recent downloads, echo=FALSE, warning=FALSE}

if (nrow(downloads_new_all) > 0) {
  downloads_new_all <- merge(downloads_new_all, gen_locs, by = "recv")
  downloads_new_all <- downloads_new_all[,c("recv", "start", "end", "location",
                                            "sun_volts", "bat_volts")]
  kable(downloads_new_all, row.names = F)
} else {
  "No new downloads"
}

````

<br/>

## Corrupted Downloads

```{r print incomplete downloads}

colnames(corrupted_downloads) <- c("Filename", "Download_attempts", "filename_long")

kable(corrupted_downloads[,c("Filename", "Download_attempts")], row.names = F)
```


<br/>

## Daily Detection Efficiency from Beacon Tags

<br/>

Not currently available
```{r print figures of detection efficiency, echo=FALSE, warning=FALSE, message=FALSE,  results = "hide"}

# library(ggplot2)
# 
# setwd(paste(file.path(Sys.getenv("USERPROFILE"),"Desktop",fsep="\\"), "\\Real-time data massaging\\products", sep = ""))
# 
# detects <- fread("detects_beacon.csv", stringsAsFactors = FALSE)
# 
# detects$DateTime_PST <- as.POSIXct(detects$DateTime_PST, format = "%Y-%m-%d %H:%M:%OS")
# 
# 
# detects$Day <- as.Date(detects$DateTime_PST, "Etc/GMT+8")
# detects <- as.data.frame(detects)
# detects$beacon <- NA
# detects[detects$recv == 157002, "beacon"] <- "FADA"
# detects[detects$recv == 157003, "beacon"] <- "F96A"
# detects[detects$recv == 157004, "beacon"] <- "FCAA"
# detects[detects$recv == 157005, "beacon"] <- "FA55"
# detects_beacon <- detects[detects$beacon==detects$TagCode,]
# 
# ## Remove multipaths
# detects_beacon_no_mp <- NULL
# 
# for (i in c("FADA", "F96A", "FCAA", "FA55")){
#   temp <- detects_beacon[detects_beacon$TagCode == i,]
#   temp <- temp[order(temp$DateTime_PST),]
#   temp$diff <- shift(temp$DateTime_PST, n=1, fill=NA, type="lag")
#   temp$difftime <- difftime(temp$DateTime_PST, temp$diff, units = "secs")
#   temp_final <- temp[temp$difftime > 1,]
#   detects_beacon_no_mp <- rbind(detects_beacon_no_mp, temp_final)
# }
# 
# daily_beacon <- aggregate(list(tag_count = detects_beacon_no_mp$location), by = list(recv = detects_beacon_no_mp$recv, day = detects_beacon_no_mp$Day, tag = detects_beacon_no_mp$TagCode), FUN = length)
# 
# daily_beacon[daily_beacon$tag == "FADA","expected_count"] <- 24*60*(60/62.4)
# daily_beacon[daily_beacon$tag == "F96A","expected_count"] <- 24*60*(60/62.2)
# daily_beacon[daily_beacon$tag == "FCAA","expected_count"] <- 24*60*(60/62.5)
# daily_beacon[daily_beacon$tag == "FA55","expected_count"] <- 24*60*(60/64)
# 
# daily_beacon$percent_detect <- daily_beacon$tag_count/daily_beacon$expected_count * 100
# 
# Tower <- daily_beacon[daily_beacon$recv %in% c(157002,157004),]
# I80 <- daily_beacon[daily_beacon$recv %in% c(157003,157005),]
# 
# ggplot(Tower, aes(day, percent_detect, fill = factor(recv))) + 
#   geom_bar(stat="identity", position = "dodge") + 
#   scale_fill_brewer(palette = "Set1") +
#   xlab("Date") +
#   ylab("% received beacon transmissions") +
#   scale_y_continuous(limits = c(0,100)) +
#   labs(fill='Receiver') +
#   ggtitle("Tower Bridge Receiver Detection Efficiency")
# 
# ggplot(I80, aes(day, percent_detect, fill = factor(recv))) + 
#   geom_bar(stat="identity", position = "dodge") + 
#   scale_fill_brewer(palette = "Set1") +
#   xlab("Date") +
#   ylab("% received beacon transmissions") +
#   scale_y_continuous(limits = c(0,100)) +
#   labs(fill='Receiver') +
#   ggtitle("I80 Bridge Receiver Detection Efficiency")

```
<br/>

## Fish Arrivals per Surgeon

<br/>


```{r see winter run counts per tagger, echo=FALSE}

print("not currently available")
# setwd(paste(file.path(Sys.getenv("USERPROFILE"),"Desktop",fsep="\\"), "\\Real-time data massaging\\products", sep = ""))
# 
# 
# detects <- read.csv("detects_Winter_H_2018.csv", stringsAsFactors = FALSE)
# tagging <- read.csv("Tagged_Fish_Metadata.txt", stringsAsFactors = F)
# tagging <- tagging[tagging$StudyID == "Winter_H_2018",]
# 
# detects_tagger <- merge(detects, tagging[,c("TagID_Hex","Surgeon", "Rel_group")], by.x = "TagCode", by.y = "TagID_Hex")
# 
# arrivals <- as.data.frame(table(detects_tagger[!duplicated(detects_tagger$TagCode),"Surgeon"]))
# 
# colnames(arrivals) <- c("Surgeon", "Unique_Fish_Arrivals")
# arrivals$total_tagged <- 200
# arrivals$percent_arrivals <- round(arrivals$Unique_Fish_Arrivals/arrivals$total_tagged * 100,2)
# 
# kable(arrivals)


```


