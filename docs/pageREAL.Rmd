---
title: CalFishTrack

output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    includes:
      in_header: GA_Script.html
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(knitr)
library(kableExtra)
library(lubridate)
#library(xtable)
#library(prettydoc)
library(scales)
library(viridis)


```

#  *Central Valley Enhanced*
#  *Acoustic Tagging Project*
```{r logos, echo=FALSE, cache=TRUE}
htmltools::img(src = knitr::image_uri("../data/logos.jpg"), 
               alt = 'logo', 
               style = 'position:absolute; top:10px; right:0px; width:200px;')
```

<br/>


# *Real-time tracking Diagnostics*


```{r bring in data, massage, create new data files, print diagnostics, echo=FALSE, warning=FALSE, message=FALSE, results = "hide"}

library(knitr)
library(dplyr)
library(data.table)
library(lubridate)

dirs <- list.dirs(path= "C:/Users/field/Desktop/ATS_Data", recursive = F)
setwd(paste(file.path(Sys.getenv("USERPROFILE"),"Desktop",fsep="\\"), "\\Real-time data massaging\\products", sep = ""))

ATS_processed_files <- fread("ATS_processed_files.csv", stringsAsFactors = F)
TECKNO_processed_files <- fread("TECKNO_processed_files.csv", stringsAsFactors = F, sep = ",")
UCD_processed_files <- fread("UCD_processed_files.csv", stringsAsFactors = F)
gen_locs <- read.csv("realtime_locs.csv", stringsAsFactors = F)
gen_locs$recv <- as.character(gen_locs$recv)
gen_locs$start <- as.POSIXct(gen_locs$start, tz = "Etc/GMT+8")
gen_locs$stop <- as.POSIXct(gen_locs$stop, tz = "Etc/GMT+8")
beacon_deployments <- read.csv("beacon_deployments.csv", stringsAsFactors = F)
beacon_deployments$start <- as.POSIXct(beacon_deployments$start, tz = "Etc/GMT+8")
beacon_deployments$stop <- as.POSIXct(beacon_deployments$stop, tz = "Etc/GMT+8")
beacon_deployments$recv <- as.character(beacon_deployments$recv)

beacon_by_day <- fread("beacon_by_day.csv", stringsAsFactors = F)
## Remove the beacon assignment, since this gets recalculated at each run
beacon_by_day$beacon <- NULL

downloads <- read.csv("downloads.csv", stringsAsFactors = FALSE)
downloads$end <- ymd_hms(downloads$end, tz = "Etc/GMT+8")
downloads$start <- ymd_hms(downloads$start, tz = "Etc/GMT+8")
## remove all gen loc details from downloads, that way these get re-assigned every hour in case gen_loc data changes. This gets reassigned at the end of this code before exporting
downloads <- downloads[,-which(names(downloads) %in% c("location", "general_location", "latitude", "longitude", "rkm"))]

corrupted_downloads <- read.csv("corrupted_downloads.csv", stringsAsFactors = F)
tagcodes <- fread("qry_HexCodes.txt", stringsAsFactors = F)
tagcodes$RelDT <- as.POSIXct(tagcodes$RelDT, format = "%m/%d/%Y %I:%M:%S %p", tz = "Etc/GMT+8")

shed <- read.csv(file = "shed_tags.csv", stringsAsFactors = F)

detects <- fread("detects_final.csv", stringsAsFactors = FALSE)
detects_beacon <- fread("detects_beacon.csv", stringsAsFactors = FALSE)

#detects$DateTime_PST <- as.POSIXct(detects$DateTime_PST, format = "%Y-%m-%d %H:%M:%OS ", tz = "Etc/GMT+8")
#detects_beacon$DateTime_PST <- as.POSIXct(detects_beacon$DateTime_PST, format = "%Y-%m-%d %H:%M:%OS ", tz = "Etc/GMT+8")
detects$DateTime_PST <- ymd_hms(detects$DateTime_PST, tz = "Etc/GMT+8")
detects_beacon$DateTime_PST <- ymd_hms(detects_beacon$DateTime_PST, tz = "Etc/GMT+8")

## At this point, let's remove any detects_beacon data that is older than 30 days and stick it in a weekly detects_beacon file. This will keep the amount of detections that gets uploaded into this script low so as to keep it running efficiently
## first, upload the time of last weekly data dump, and run this operation if it hasn't happened in a week
weekly_dump <- read.csv("weekly_data_dump.csv")
weekly_dump <- as.POSIXct(weekly_dump$x, format = "%Y-%m-%d %H:%M:%S")
if(difftime(Sys.time(),weekly_dump, units = "days") > 7){
  ## Find time exactly 30 days ago
  dump.end <- (Sys.time() - (30*24*60*60))
  ## Extract and export detections up to that date, making sure to save datetime as character to save milliseconds
  data_dump <- detects_beacon[detects_beacon$DateTime_PST < dump.end,]
  data_dump$DateTime_PST <- format(data_dump$DateTime_PST, "%Y-%m-%d %H:%M:%OS6")
  fwrite(data_dump, paste("C:/Users/field/Desktop/Real-time data massaging/products/Detects_beacon_old/weekly_detects_beacon_ending_", as.Date(dump.end), ".csv", sep = ""), row.names = F)
  ## Remove those records from the detects_beacon dataframe
  detects_beacon <- detects_beacon[!detects_beacon$DateTime_PST < dump.end,]
  ## Now save current datetime if a dump occured
  write.csv(Sys.time(), "weekly_data_dump.csv", row.names = F)
}

## Now remove all gen loc data from detections in case genloc details have changed. We will add genloc data back in at the end of this code
detects <- subset(detects, select= -c(location, general_location, latitude, longitude, rkm))
detects_beacon <- subset(detects_beacon, select= -c(location, general_location, latitude, longitude, rkm))


## Find all Teckno file names
TECKNO_all_files <- as.data.frame(list(x=dir(pattern = "*.txt", path = "C:/Users/field/Desktop/Tekno_data/Dropbox (Teknologic LLC)/Apps/SRCOM/NOAA.SC", full.names = T, recursive = T)), stringsAsFactors = F)
## Select only newest TECKNO files for massaging
TECKNO_files <- as.data.frame(list(x=TECKNO_all_files[!TECKNO_all_files$x %in% TECKNO_processed_files$x,]), stringsAsFactors = F)
## Also, remove all the older files that google drive messed up
TECKNO_files$dayes <- as.Date(apply(data.frame(TECKNO_files$x, stringsAsFactors = F), MARGIN = 1, FUN = substr, 93, 102), "%Y_%m_%d")
TECKNO_files <- TECKNO_files[TECKNO_files$dayes > "2019-02-10",]
TECKNO_files$dayes <- NULL


## Remove bad files that don't contain real data from list for processing
bad_files <- read.csv("C:/Users/field/Desktop/Real-time data massaging/products/bad_files.csv", colClasses = "character")
# TECKNO_files <- data.frame(x=TECKNO_files[-grep(paste(bad_files$V1, collapse = "|"), TECKNO_files$x),], stringsAsFactors = F)


## Find all ATS file names
ATS_all_files <- as.data.frame(list(x=dir(pattern = ".csv", path = dirs, full.names = T)), stringsAsFactors = F)
## Select only newest ATS files for massaging
ATS_files <- as.data.frame(list(x=ATS_all_files[!ATS_all_files$x %in% ATS_processed_files$x,]), stringsAsFactors = F)
## Remove bad files that don't contain real data from list for processing
## Bad_files may also be files that have timezone issues that are almost untractable
ATS_files <- subset(ATS_files, !grepl(paste(bad_files$filename, collapse = "|"), ATS_files$x))
#ATS_files <- as.data.frame(list(x=ATS_files[!which(grep(paste(bad_files$filename, collapse = "|"), ATS_files$x)),]), stringsAsFactors = F)
## Load CSt timezone file list
CST_files <- read.csv("C:/Users/field/Desktop/Real-time data massaging/products/receiver_files_with_CST_timezones.csv", colClasses = "character")


## Find all UCD file names
UCD_all_files <- as.data.frame(list(x=dir(pattern = ".csv", path = "C:/Users/field/Desktop/UCD_files", full.names = T)), stringsAsFactors = F)
## Only keep "_awked.csv" files
UCD_all_files <- as.data.frame(list(x=UCD_all_files[substr(UCD_all_files$x, nchar(UCD_all_files$x)-8, nchar(UCD_all_files$x)-4) == "awked",]), stringsAsFactors = F)
## Select only newest ATS files for massaging
UCD_files <- as.data.frame(list(x=UCD_all_files[!UCD_all_files$x %in% UCD_processed_files$x,]), stringsAsFactors = F)


#### Data massage ATS files first ####

downloads_new_all <- data.frame()

if (nrow(ATS_files) > 0) {
  
  ## First, find shortname for ATS files
  shortname <- regexpr(ATS_files$x, pattern = "Data")[1]
  incomplete_shortname <- substr(ATS_files$x, shortname+5, nchar(ATS_files$x))
  shortname <- strsplit(incomplete_shortname, "/")
  ATS_files$shortname <- unlist(lapply(shortname, tail, 1))
  
  downloads_new1 <- data.frame()
  
  
  for (i in 1:nrow(ATS_files)) {
    if (file.info(ATS_files[i,"x"])$size >0){    
      if (substr(read.csv(ATS_files[i,"x"], nrows = 1, header = F)[,1] , 1,4) == "Site") {
        hour <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, skip = 7, nrows = 1, header = F)
        recv <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, skip = 2, nrows = 1, header = F)
      }  
      if (substr(read.csv(ATS_files[i,"x"], nrows = 1, header = F)[,1] , 1,4) == " Num") {
        recv <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, nrows = 1, header = F)
        hour <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, skip = 13, nrows = 1, header = F)
      }else{
        recv <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, skip = 6, nrows = 1, header = F)
        hour <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, skip = 19, nrows = 1, header = F)
      }
      hour <- substr(hour, start = 13, stop = 31)
      recv <- as.data.frame(strsplit(as.character(recv),": "), stringsAsFactors = F)[2,]
      downloads_new1 <- rbindlist(list(downloads_new1, 
                                       data.frame(filename = ATS_files[i, "shortname"],recv, hour, stringsAsFactors = F)))#, use.names = F, fill = F, idcol = "filename")
    }
  }
  
  downloads_new1$start <- as.POSIXct(downloads_new1$hour, format = "%m/%d/%Y %H:%M:%S", tz = "Etc/GMT+8")
  
  ## Now change downloads start and end to correct time since some are on CST and some are PST + 1 day
  
  for (i in 1:nrow(downloads_new1)){ 
    if(downloads_new1[i, "filename"] %in% CST_files$Filenames) {
      downloads_new1[i, "start"] <- downloads_new1[i, "start"]-2*60*60
    }
  }
  
  downloads_new1$end <- downloads_new1$start +(60*60)
  
  ## Add dummy columns to match the ones from Tecknos that actually carry data
  downloads_new1$sun_volts <- NA
  downloads_new1$bat_volts <- NA
  
  ## Here, remove files that didn't have their download data read correctly and add them to the corrupted files list. These files should not be read for detections
  short_files <- ATS_files[ATS_files$shortname %in% downloads_new1[downloads_new1$recv == "","filename"],]
  if(nrow(short_files)){  
    downloads_new1 <- downloads_new1[!downloads_new1$recv == "",]
    ATS_files <- ATS_files[!ATS_files$shortname %in% short_files$shortname,]
    for (i in 1:nrow(short_files)){
      if(short_files[i, "shortname"] %in% corrupted_downloads$V1){
        corrupted_downloads[corrupted_downloads$V1 == short_files[i, "shortname"], "download_attempts"] <- 
          as.numeric(corrupted_downloads[corrupted_downloads$V1 == short_files[i, "shortname"], "download_attempts"])+1
      }else{
        corrupted_downloads <- rbind(corrupted_downloads, cbind(V1 = short_files[i, "shortname"], download_attempts = 1, filename = short_files[i,"x"]))
      }
    }
  }
  
  ## Now gather detections
  detects_temp <- data.frame()
  
  for (i in 1:nrow(ATS_files)) {
    ## First, put in error catch for files that are completely empty
    
    if (file.info(ATS_files[i,"x"])$size >0){
      if (substr(read.csv(ATS_files[i,"x"], nrows = 1, header = F)[,1] , 1,4) == "Site") {
        test <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, skip = 9, header = T,  row.names=NULL)
      }  
      if (substr(read.csv(ATS_files[i,"x"], nrows = 1, header = F)[,1] , 1,4) == " Num") {
        test <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, skip = 16, header = T,  row.names=NULL)
      }else{
        test <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, skip = 22, header = T,  row.names=NULL)
      }
      
      if(colnames(test)[1] != "Internal"){
        colnames(test) <- colnames(test)[-1]
        test[,ncol(test)] <- NULL
      }
      ## Find file end, if it exists (if it doesn't, file is corrupted, see below)
      end1 <- head(grep("--", test[,1]),1)
      end2 <- head(grep("File End", test[,1]),1)
      end_final <- head(grep(".-- Done", test[,1]),1)
    }
    
    ## This is another error catcher for weird ATS files that are corrupted
    trytest <- try(as.POSIXct(test[is.na(test$DateTime)==F,"DateTime"], format = "%m/%d/%Y %H:%M:%OS", tz = "Etc/GMT+8"), silent = T)
    
    if (length(end_final) == 0 | file.info(ATS_files[i,"x"])$size == 0 | inherits(trytest, "try-error") | nrow(test[is.na(test$DateTime)==F,]) == 0) {
      ## This means the detection file is incomplete aka corrupted
      ## Or, the second term above tests if the file is completely empty, aka also corrupted
      ## If the file is corrupted and new, add to corrupted list
      ## if the file is corrupted and a repeat, tally 1 to the counter
      ## At the end of this code, Remove the file from the downloads table
      ## And physically remove the file from the computer if the tally counter is <5
      ## If >= 5, leave the file as this will avoid having to download it again to no avail
      ## We do this at the end so we don't delete files when prototyping this ATS code segment
      downloads_new1 <- downloads_new1[!downloads_new1$filename == ATS_files[i,"shortname"],]
      if(ATS_files[i, "shortname"] %in% corrupted_downloads$V1){
        corrupted_downloads[corrupted_downloads$V1 == ATS_files[i, "shortname"], "download_attempts"] <- 
          as.numeric(corrupted_downloads[corrupted_downloads$V1 == ATS_files[i, "shortname"], "download_attempts"])+1
      }else{
        corrupted_downloads <- rbind(corrupted_downloads, cbind(V1 = ATS_files[i, "shortname"], download_attempts = 1, filename = ATS_files[i,"x"]))
      }
      
    }else{
      
      ## if a previously corrupted file did finally get read correctly, remove it from the corrupted file
      if(ATS_files[i, "shortname"] %in% corrupted_downloads$V1){
        corrupted_downloads <- corrupted_downloads[!corrupted_downloads$V1 == ATS_files[i, "shortname"],] 
      }
      ## Now data massage
      test$recv <- substr(ATS_files[i,"x"], nchar(ATS_files[i,"x"])-22, nchar(ATS_files[i,"x"])-18)
      test1 <- test[-c(end_final:nrow(test)),]
      test2 <- test1[!test1$TagCode == "",]
      test3 <- test2[!test2$TagCode == " TagCode",]
      
      test4 <- test3[!test3$TagCode == " GPS Fix  ",]
      
      names(test4)[names(test4) == 'DateTime'] <- 'DateTime_Orig'
      
      test4$DateTime_Orig <- as.POSIXct(test4$DateTime_Orig, format = "%m/%d/%Y %H:%M:%OS", tz = "Etc/GMT+8")
      
      test4$DateTime_PST <- test4$DateTime_Orig
      
      ## Now change DateTime_PST to PST, but this is tricky because for some, orig time is in CST
      ## Have this first if statement in case there are no detections in a file
      if(nrow(test4)>0){
        if(ATS_files[i,"x"] %in% CST_files$Filenames) {
          test4$DateTime_PST <- test4$DateTime_PST-2*60*60
          attributes(test4$DateTime_PST)$tzone <- "Etc/GMT+8"
        }
        
        ## Finally, if erroneous receiver serial number "09000"" comes up, change it to 18092
        test4[test4$recv == "09000", "recv"] <- "18092"
        
        ## change Datetime_Orig with milliseconds shown (this turns the class into character)
        test4$DateTime_Orig <- format(test4$DateTime_Orig, "%Y-%m-%d %H:%M:%OS6")
        
        detects_temp <- rbindlist(list(detects_temp, test4))
        
        
      }
    }
  }
  
  
  downloads_new_all <- rbindlist(list(downloads_new_all, downloads_new1[,c("filename", "recv", "hour", "sun_volts", "bat_volts", "end", "start")]))
  
  ## In case all new files were corrupted i.e. length(detects_temp) = 0, have this if statement
  if (length(detects_temp) > 0){
    ## Now remove prefix and suffix of TagCode
    detects_temp$TagCode <- substr(detects_temp$TagCode, start = 5, stop = 8)
    
    ## Remove "ffff" detections
    detects_temp <- detects_temp[!detects_temp$TagCode == "ffff",]
    
    ## Make temp numeric
    detects_temp$Temp <- as.numeric(detects_temp$Temp)
    
    ## Make sure critical rows are NA, this has been happening on rare occasions
    detects_temp <- detects_temp[!(is.na(detects_temp$recv) | is.na(detects_temp$DateTime_PST) | is.na(detects_temp$TagCode)),]
    
    ## Sometimes weird negative or 99.99 degree temp recordings are logged, remove these
    detects_temp[detects_temp$Temp > 99 | detects_temp$Temp < 0, "Temp"] <- NA
    
    ## A few receivers seem to have had temp sensors, so make NA
    #detects_temp[detects_temp$recv %in% c(17127, 17139, 17143, 17147, 18008, 18003, 17144, 18002, 18006, 18007, 18092), "Temp"] <- NA
    
    ## Here, put only non-beacon detections in detection file
    detects_new <- detects_temp[!detects_temp$TagCode %in% c(unique(beacon_deployments$beacon),"00CA","00C9", "F6AC", "006F", "006D"),]
    ## And put beacon detections into seperate file
    detects_beacon_new <- detects_temp[detects_temp$TagCode %in% c(unique(beacon_deployments$beacon),"00CA","00C9", "F6AC", "006F", "006D"),]
    
    ## Now remove multipath detections from detects_beacon_new
    ## First reorder the data since the above steps may have jumbled things
    detects_beacon_new <- detects_beacon_new[order(detects_beacon_new$recv, detects_beacon_new$TagCode, detects_beacon_new$DateTime_PST),]
    detects_beacon_new$time_to_next <- as.numeric(difftime(shift(detects_beacon_new$DateTime_PST, fill = NA, type = "lead"), detects_beacon_new$DateTime_PST, units = "hours"))
    detects_beacon_new$time_from_previous <- as.numeric(difftime(detects_beacon_new$DateTime_PST, shift(detects_beacon_new$DateTime_PST, fill = NA, type = "lag"),  units = "hours"))
    ## Now make NA the time diff values when it's between 2 different recvs or tagcodes
    detects_beacon_new[which(detects_beacon_new$recv != shift(detects_beacon_new$recv, fill = NA, type = "lag")), "time_from_previous"] <- NA
    detects_beacon_new[which(detects_beacon_new$recv != shift(detects_beacon_new$recv, fill = NA, type = "lead")), "time_to_next"] <- NA
    detects_beacon_new[which(detects_beacon_new$TagCode != shift(detects_beacon_new$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
    detects_beacon_new[which(detects_beacon_new$TagCode != shift(detects_beacon_new$TagCode, fill = NA, type = "lead")), "time_to_next"] <- NA
    ## Now remove the second detection every time the detection lag difftime is less than 0.3 sec (multipath)
    detects_beacon_new <- detects_beacon_new[detects_beacon_new$time_from_previous > 0.000083 | is.na(detects_beacon_new$time_from_previous) == TRUE ,]
    
    #detects_new <- merge(detects_new, coords, by = "recv", all.x = T)
    
    #detects_new <- merge(detects_new, gen_locs, by = "recv", all.x = T)
    
    detects_beacon <- rbind(detects_beacon, detects_beacon_new[,c("recv", "DateTime_Orig", "TagCode", "DateTime_PST", "Temp")])
    detects <- rbind(detects, detects_new[,c("recv", "DateTime_Orig", "TagCode", "DateTime_PST", "Temp")])
  }  
}



#######################################
#### Now data massage TECKNO files ####
#######################################




if (nrow(TECKNO_files) > 0) {
  downloads_new2 <- data.frame()
  for (i in 1:nrow(TECKNO_files)) {
    test <- try(read.csv(TECKNO_files[i,], header = F, stringsAsFactors = F ), silent = T) 
    if (inherits(test, "try-error") | ifelse(is.null(ncol(test)) ==T, TRUE, ncol(test) < 18)) {
      next
    } else {
      test1 <- tail(test[nchar(test$V5) > 4,],1)
      hour <- test1$V3
      recv <- test1$V1
      ## Get volts, sometimes in row 1 or 2
      sun_volts <- max(test[1:2,"V17"])
      bat_volts <- max(test[1:2,"V18"])
      downloads_new2 <- rbindlist(list(downloads_new2, data.frame(list(filename = TECKNO_files[i,], recv=recv, hour=hour, sun_volts = sun_volts, bat_volts = bat_volts), stringsAsFactors = F)))
    }
  }
  
  downloads_new2$end <- as.POSIXct(downloads_new2$hour, tz = "GMT")
  downloads_new2$start <- downloads_new2$end -(60*60)
  
  
  
  ## Now change all times to PST to match ATS receivers
  attributes(downloads_new2$start)$tzone <- "Etc/GMT+8"
  attributes(downloads_new2$end)$tzone <- "Etc/GMT+8"
  
  downloads_new_all <- rbindlist(list(downloads_new_all, downloads_new2))
  
  detects_temp <- data.frame()
  
  ## Put in progress bar
  #pb <- txtProgressBar(min = 0, max = nrow(TECKNO_files), style = 3)
  
  for (i in 1:nrow(TECKNO_files)) {
    res <- try(read.csv(TECKNO_files[i,], header = F), silent = T) 
    if (inherits(res, "try-error")) {
      corrupted <- strsplit(TECKNO_files[i,], "/")
      corrupted2 <- unlist(lapply(corrupted, tail, 1))
      corrupted_downloads <- rbind(corrupted_downloads,cbind(V1 = corrupted2, download_attempts = 3, filename = TECKNO_files[i,]))
      ## Also make corrupted downloads more palateable
      next
    } else {
      start <- nchar(read.csv(TECKNO_files[i,],stringsAsFactors = F, header = F)$V5)
      start <- head(which(start==4),1)
      if (length(start) > 0) {
        test <- read.csv(TECKNO_files[i,], skip = start-1, header = F, stringsAsFactors = F )
        test1 <- test[nchar(test$V5)==4,]
        detects_temp <- rbindlist(list(detects_temp, test1[,1:9]))
        
      }    
    }
    #setTxtProgressBar(pb, i)
  }
  #close(pb)
  
  colnames(detects_temp) <- c("recv", "V2", "DateTime_Orig", "dec_seconds", "TagCode", "V6", "V7", "V8", "V9")
  detects_temp <- detects_temp[!detects_temp$V2 == "N",]
  ## In this step, append datetime to milliseconds, but make sure the 6 decimal places are represented for all cases, even when there are trailering zeros
  detects_temp$DateTime_Orig <- paste(detects_temp$DateTime_Orig, substr(formatC(as.numeric(detects_temp$dec_seconds),format='f', digits=6 ),2,8), sep = "")
  ## Make a POSIX class, note that TECKNO detections are in GMT, will need to convert to local time
  detects_temp$DateTime_PST <- as.POSIXct(detects_temp$DateTime_Orig, format = "%Y-%m-%d %H:%M:%OS", tz = "GMT")
  attributes(detects_temp$DateTime_PST)$tzone <- "Etc/GMT+8"
  
  ## Add column for temperature
  detects_temp$Temp <- NA
  
  ## Here, put only non-beacon detections in detection file
  detects_new <- detects_temp[!detects_temp$TagCode %in% c(unique(beacon_deployments$beacon),"00CA","00C9", "F6AC", "006F", "006D"),]
  ## And put beacon detections into seperate file
  detects_beacon_new <- detects_temp[detects_temp$TagCode %in% c(unique(beacon_deployments$beacon),"00CA","00C9", "F6AC", "006F", "006D"),]
  
  ## Now remove multipath detections from detects_beacon_new
  ## First reorder the data since the above steps may have jumbled things
  detects_beacon_new <- detects_beacon_new[order(detects_beacon_new$recv, detects_beacon_new$TagCode, detects_beacon_new$DateTime_PST),]
  detects_beacon_new$time_to_next <- as.numeric(difftime(shift(detects_beacon_new$DateTime_PST, fill = NA, type = "lead"), detects_beacon_new$DateTime_PST, units = "hours"))
  detects_beacon_new$time_from_previous <- as.numeric(difftime(detects_beacon_new$DateTime_PST, shift(detects_beacon_new$DateTime_PST, fill = NA, type = "lag"),  units = "hours"))
  ## Now make NA the time diff values when it's between 2 different recvs or tagcodes
  detects_beacon_new[which(detects_beacon_new$recv != shift(detects_beacon_new$recv, fill = NA, type = "lag")), "time_from_previous"] <- NA
  detects_beacon_new[which(detects_beacon_new$recv != shift(detects_beacon_new$recv, fill = NA, type = "lead")), "time_to_next"] <- NA
  detects_beacon_new[which(detects_beacon_new$TagCode != shift(detects_beacon_new$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
  detects_beacon_new[which(detects_beacon_new$TagCode != shift(detects_beacon_new$TagCode, fill = NA, type = "lead")), "time_to_next"] <- NA
  ## Now remove the second detection every time the detection lag difftime is less than 0.3 sec (multipath)
  detects_beacon_new <- detects_beacon_new[detects_beacon_new$time_from_previous > 0.000083 | is.na(detects_beacon_new$time_from_previous) == TRUE ,]
  
  ## Now append these to existing datasets
  detects <- rbindlist(list(detects, detects_new[,c("recv", "DateTime_Orig", "TagCode", "DateTime_PST", "Temp")]))
  detects_beacon <- rbindlist(list(detects_beacon, detects_beacon_new[,c("recv", "DateTime_Orig", "TagCode", "DateTime_PST", "Temp")]))
  
}


#######################################
#### Now data massage UCD files ####
#######################################
if (nrow(UCD_files) > 0) {
  downloads_new3 <- data.frame()
  for (i in 1:nrow(UCD_files)) {
    test <- try(read.csv(UCD_files[i,], header = F, stringsAsFactors = F ), silent = T) 
    if (inherits(test, "try-error")) {
      next
    } else {
      
      hour <- substr(UCD_files[i,], nchar(UCD_files[i,])-21, nchar(UCD_files[i,])-10)
      recv <- substr(UCD_files[i,], nchar(UCD_files[i,])-28, nchar(UCD_files[i,])-23)
      ## Get volts, sometimes in row 1 or 2
      sun_volts <- NA
      bat_volts <- NA
      downloads_new3 <- rbindlist(list(downloads_new3, data.frame(list(filename = UCD_files[i,], recv=recv, hour=hour, sun_volts = sun_volts, bat_volts = bat_volts), stringsAsFactors = F)))
    }
  }
  
  if (nrow(downloads_new3) > 0) {
    downloads_new3$end <- as.POSIXct(downloads_new3$hour, tz = "GMT", format = "%Y%m%d%H%M")
    downloads_new3$start <- downloads_new3$end -(2*60*60)
    
    
    
    ## Now change all times to PST to match ATS receivers
    attributes(downloads_new3$start)$tzone <- "Etc/GMT+8"
    attributes(downloads_new3$end)$tzone <- "Etc/GMT+8"
    
    downloads_new_all <- rbindlist(list(downloads_new_all, downloads_new3))
    
    detects_temp <- data.frame()
    
    for (i in 1:nrow(UCD_files)) {
      res <- try(read.csv(UCD_files[i,], header = F), silent = T) 
      if (inherits(res, "try-error")) {
        corrupted <- strsplit(UCD_files[i,], "/")
        corrupted2 <- unlist(lapply(corrupted, tail, 1))
        corrupted_downloads <- rbind(corrupted_downloads,cbind(V1 = corrupted2, download_attempts = 5, filename = UCD_files[i,]))
        ## Also make corrupted downloads more palateable
        next
      } else {
        detects_temp <- rbindlist(list(detects_temp, res))
      }
      
    }
    
    
    colnames(detects_temp) <- c("recv", "DateTime_Orig", "TagCode")
    ## Make a POSIX class, note that UCD detections are in GMT, will need to convert to local time
    detects_temp$DateTime_PST <- as.POSIXct(detects_temp$DateTime_Orig, format = "%Y-%m-%d %H:%M:%OS", tz = "GMT")
    attributes(detects_temp$DateTime_PST)$tzone <- "Etc/GMT+8"
    
    ## Add column for temperature
    detects_temp$Temp <- NA
    
    ## Here, put only non-beacon detections in detection file
    detects_new <- detects_temp[!detects_temp$TagCode %in% c(unique(beacon_deployments$beacon), "006F", "006D", "F6AC", "00CA", "00C9"),]
    ## And put beacon detections into seperate file
    detects_beacon_new <- detects_temp[detects_temp$TagCode %in% c(unique(beacon_deployments$beacon), "006F", "F6AC", "006D", "00CA", "00C9"),]
    
    ## Now remove multipath detections from detects_beacon_new
    ## First reorder the data since the above steps may have jumbled things
    detects_beacon_new <- detects_beacon_new[order(detects_beacon_new$recv, detects_beacon_new$TagCode, detects_beacon_new$DateTime_PST),]
    detects_beacon_new$time_to_next <- as.numeric(difftime(shift(detects_beacon_new$DateTime_PST, fill = NA, type = "lead"), detects_beacon_new$DateTime_PST, units = "hours"))
    detects_beacon_new$time_from_previous <- as.numeric(difftime(detects_beacon_new$DateTime_PST, shift(detects_beacon_new$DateTime_PST, fill = NA, type = "lag"),  units = "hours"))
    ## Now make NA the time diff values when it's between 2 different recvs or tagcodes
    detects_beacon_new[which(detects_beacon_new$recv != shift(detects_beacon_new$recv, fill = NA, type = "lag")), "time_from_previous"] <- NA
    detects_beacon_new[which(detects_beacon_new$recv != shift(detects_beacon_new$recv, fill = NA, type = "lead")), "time_to_next"] <- NA
    detects_beacon_new[which(detects_beacon_new$TagCode != shift(detects_beacon_new$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
    detects_beacon_new[which(detects_beacon_new$TagCode != shift(detects_beacon_new$TagCode, fill = NA, type = "lead")), "time_to_next"] <- NA
    ## Now remove the second detection every time the detection lag difftime is less than 0.3 sec (multipath)
    detects_beacon_new <- detects_beacon_new[detects_beacon_new$time_from_previous > 0.000083 | is.na(detects_beacon_new$time_from_previous) == TRUE ,]
    
    ## Now append these to existing datasets
    detects <- rbind(detects, detects_new[,c("recv", "DateTime_Orig", "TagCode", "DateTime_PST", "Temp")])
    detects_beacon <- rbind(detects_beacon, detects_beacon_new[,c("recv", "DateTime_Orig", "TagCode", "DateTime_PST", "Temp")])
    
  }
}


## Now remove duplicate detections that may have occured when an hourly file was inadvertantly uploaded twice
detects <- detects[!duplicated(detects[,c("recv", "DateTime_Orig", "TagCode")])]
detects_beacon <- detects_beacon[!duplicated(detects_beacon[,c("recv", "DateTime_Orig", "TagCode")])]

## Now merge location data back with detections and detects_beacon

## Now assign the location name and other location data based on the start and stop time in gen_locs
## Note that this will assign NA for all location data if a detection doesn't get assigned to a location
for (i in 1:nrow(gen_locs)){
  detects[which(gen_locs$recv[i] == detects$recv & gen_locs$start[i] <= detects$DateTime_PST & (gen_locs$stop[i] > detects$DateTime_PST | is.na(gen_locs$stop[i]))),c("location", "general_location", "latitude", "longitude", "rkm")] <- gen_locs[i, c("location", "general_location", "latitude", "longitude", "rkm")]
}

for (i in 1:nrow(gen_locs)){
  detects_beacon[which(gen_locs$recv[i] == detects_beacon$recv & gen_locs$start[i] <= detects_beacon$DateTime_PST & (gen_locs$stop[i] > detects_beacon$DateTime_PST | is.na(gen_locs$stop[i]))),c("location", "general_location", "latitude", "longitude", "rkm")] <- gen_locs[i, c("location", "general_location", "latitude", "longitude", "rkm")]
}

## Merge new downloads with exist dataset
downloads <- rbindlist(list(downloads, downloads_new_all))

## Merge in gen loc data now for all downloads
downloads[,c("location", "general_location", "latitude", "longitude", "rkm")] <- as.character(NA)

for (i in 1:nrow(gen_locs)){
  downloads[which(gen_locs$recv[i] == downloads$recv & gen_locs$start[i] <= downloads$start & (gen_locs$stop[i] > downloads$end | is.na(gen_locs$stop[i]))),c("location", "general_location", "latitude", "longitude", "rkm")] <- gen_locs[i,c("location", "general_location", "latitude", "longitude", "rkm")]
}


gap_percent <- aggregate(list(total_files = downloads$start), by = list(location = downloads$location), FUN = length) 
gap_percent <- merge(gap_percent,aggregate(list(recv_start = downloads$start), by = list(location = downloads$location), FUN = min), by = "location", all.y = T)
## Some receivers turned hourly recording off soon after deployment, meaning there is an initial gap in the data, so I'm setting the true start time to when it started uninterrupted recording (i.e. after the gap)
gap_percent[gap_percent$location == "I80-50_Br1", "recv_start"] <- as.POSIXct("2018-05-14 10:00:00")
gap_percent[gap_percent$location == "RT_OldRiver", "recv_start"] <- as.POSIXct("2018-04-18 15:00:00")
gap_percent[gap_percent$location == "GeorgSlRT_1.2", "recv_start"] <- as.POSIXct("2018-04-17 9:00:00")
gap_percent[gap_percent$location == "GeorgSlRT_1.1", "recv_start"] <- as.POSIXct("2018-04-17 4:00:00")
gap_percent[gap_percent$location == "BeniciaRT_14", "recv_start"] <- as.POSIXct("2018-04-24 18:00:00")
gap_percent[gap_percent$location == "BeniciaRT_02", "recv_start"] <- as.POSIXct("2018-04-25 17:00:00")
gap_percent[gap_percent$location == "DCCRT_1", "recv_start"] <- as.POSIXct("2018-05-16 12:00:00")
gap_percent[gap_percent$location == "GeorgSlRT_2.1", "recv_start"] <- as.POSIXct("2018-05-22 16:00:00")



## Now remove multipaths from detects_final
## Now create two columns with time in hours between the previous and next detection, for each detection
## first order data appropriately

detects <- detects[order(detects$recv, detects$TagCode, detects$DateTime_PST),]
detects$time_from_previous <- as.numeric(difftime(detects$DateTime_PST, shift(detects$DateTime_PST, fill = NA, type = "lag"),  units = "secs"))
## Now make NA the time diff values when it's between 2 different gen locs or tagcodes
detects[which(detects$recv != shift(detects$recv, fill = NA, type = "lag")), "time_from_previous"] <- NA
detects[which(detects$TagCode != shift(detects$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
## Now remove the second detection every time the detection lag difftime is less than 0.3 sec (multipath and funny behavior)
detects <- detects[detects$time_from_previous > 0.3 | is.na(detects$time_from_previous) == TRUE ,]
detects$time_from_previous <- NULL

## Now remove detections that don't meet the filtering criteria

## Now create two columns with time in hours between the previous and next detection, for each detection
## Now reorder the data since the above steps may have jumbled things
detects <- detects[order(detects$TagCode, detects$general_location, detects$DateTime_PST),]
## Now estimate the time in hours between the previous and next detection, for each detection. 
detects$time_to_next <- as.numeric(difftime(shift(detects$DateTime_PST, fill = NA, type = "lead"), detects$DateTime_PST, units = "hours"))
detects$time_from_previous <- as.numeric(difftime(detects$DateTime_PST, shift(detects$DateTime_PST, fill = NA, type = "lag"),  units = "hours"))
## Now make NA the time diff values when it's between 2 different gen locs or tagcodes
detects[which(detects$general_location != shift(detects$general_location, fill = NA, type = "lag")), "time_from_previous"] <- NA
detects[which(detects$general_location != shift(detects$general_location, fill = NA, type = "lead")), "time_to_next"] <- NA
detects[which(detects$TagCode != shift(detects$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
detects[which(detects$TagCode != shift(detects$TagCode, fill = NA, type = "lead")), "time_to_next"] <- NA

## Now mark tag detections that have a detection both before and after from the same tag at the same gen loc within 6 minute span (this seems like a good cutoff) (within 3 minutes before and 3 minutes after). We call this "filter_lag", that way it can be adjusted per studyID when necessary (in our case, it has been for green sturgeon)
## Using 179 seconds rather than 180 seconds for 3min filter to get rid of a particularly pervasive false detection
detects$before_and_after_valid <- 0
detects[is.na(detects$time_to_next) == F & is.na(detects$time_from_previous) == F & detects$time_to_next < 0.04944444 & detects$time_from_previous < 0.04944444,"before_and_after_valid"] <- 1
## In case a valid fish detection occurs right before the hour mark, we need to make sure to keep these, otherwise they might get dropped before the remaining detections come in the next hour
last_detects <- aggregate(list(last = detects$DateTime_PST), by = list(location = detects$location), FUN = max)
last_detects$last <-  last_detects$last - 15*60
detects <- merge(detects, last_detects, by = "location")
## Now reorder the data since the above steps may have jumbled things
detects <- detects[order(detects$TagCode, detects$general_location, detects$DateTime_PST),]
## Now pull out only these detections that are part of at least 3 consecutive detections that are no more than 5 minutes apart
## Also keep detections that occured within the last 15 minutes of each recv so that valid tag visits aren't dropped before all the detections come in
detects <- detects[which(shift(detects$before_and_after_valid, fill = NA, type = "lag") == 1 |
                           shift(detects$before_and_after_valid, fill = NA, type = "lead") == 1 |
                           detects$before_and_after_valid == 1 |
                           detects$DateTime_PST > detects$last),]
## Remove extra columns
detects$last <- NULL
detects$time_to_next <- NULL
detects$time_from_previous <- NULL
detects$before_and_after_valid <- NULL


#### Now remove shed tags ####
## first summarize by movement to find shed tags
detects <- detects[order(detects$TagCode, detects$DateTime_PST),]
detects$mov_count <- 0 
## Now make NA the time diff values when it's between 2 different gen locs or tagcodes
detects[which(detects$general_location != shift(detects$general_location, fill = NA, type = "lag")), "mov_count"] <- 1
## except, if between two nearby lines. Sometimes a shed tag will get detected on two lines if near both
allowed_moves <- c("Georgiana_Slough1-Georgiana_Slough2",
                   "Georgiana_Slough2-Georgiana_Slough1",
                   "Sac_BlwGeorgiana-Sac_BlwGeorgiana2",
                   "Sac_BlwGeorgiana2-Sac_BlwGeorgiana",
                   "Benicia_east-Benicia_west",
                   "Benicia_west-Benicia_east")
detects[which(paste(detects$general_location,shift(detects$general_location, fill = NA, type = "lag"), sep = "-") %in% allowed_moves), "mov_count"] <- 0
## Also make NA when between different tagcodes
detects[which(detects$TagCode != shift(detects$TagCode, fill = NA, type = "lag")), "mov_count"] <- 1
## Create movement index
detects$move <- cumsum(detects$mov_count)
## Now summarize by unique movement, find min, max, count of detections
detects_summary <- aggregate(list(min = detects$DateTime_PST), by = list(move = detects$move), FUN = min)
detects_summary <- merge(detects_summary, aggregate(list(max = detects$DateTime_PST), by = list(move = detects$move), FUN = max))
detects_summary <- merge(detects_summary, aggregate(list(n = detects$DateTime_PST), by = list(move = detects$move), FUN = length))
detects_summary$diff <- as.numeric(difftime(detects_summary$max, detects_summary$min, units = "days"))

## Bring back in relevant data from main detects table (TagCode and Genloc)
move_unique <- unique(detects[,c("move", "TagCode", "general_location")])
detects_summary <- merge(detects_summary, move_unique)

## Now pull out GenLoc visits that are longer than 30 days and have more than 20000 detections
## However, also include visits that are longer than 30 days and have been determined to be a shed tag in past runs of the script
## The majority of detections for past identified shed tags would have been pulled out, so they likely wouldn't meet 20000 condition
## However, we want to include these fish in case there are new shed detections for those fish. Those will need to be removed too
shed_new <- detects_summary[(detects_summary$diff > 30 & detects_summary$n > 20000) | (detects_summary$diff > 30 & detects_summary$TagCode %in% unique(shed$TagCode)) ,]
## Here merge the new sheds with the old sheds so that we can get the old tally of detections removed
## Also so we can identify the new move ID for old, already removed shed detections
shed_new <- merge(shed_new, shed[,c("TagCode", "general_location", "n")], by = c("TagCode", "general_location"), all = T)
## Here, find the new total given past removed detections and new removed detections
## first, remove 200 detections from old shed detection counts to account for those we kept
shed_new[is.na(shed_new$n.x)==F,"n.y"] <- shed_new[is.na(shed_new$n.x)==F,"n.y"]-200
shed_new$n <- apply(shed_new[,c("n.x", "n.y")], 1, sum, na.rm = T)
shed_new$n.x <- NULL
shed_new$n.y <- NULL

## now run a loop and pull out first and last 100 detections (200 total) that occur between the min and max of shed movements
shed_times <- unique(shed_new[,c("TagCode", "min", "max")])
shed_detects <- NULL
detects <- detects[order(detects$TagCode, detects$DateTime_PST),]
for (i in 1:nrow(shed_times)){
  shed_detects <- rbind(shed_detects, head(detects[detects$TagCode == shed_times[i,"TagCode"] & detects$DateTime_PST >= shed_times[i,"min"] & detects$DateTime_PST <= shed_times[i,"max"],],n = 100))
  shed_detects <- rbind(shed_detects, tail(detects[detects$TagCode == shed_times[i,"TagCode"] & detects$DateTime_PST >= shed_times[i,"min"] & detects$DateTime_PST <= shed_times[i,"max"],],n = 100))
}

## Now remove the shed detections, and then add back in the 100 first and last subset data
detects <- detects[!(detects$move %in% shed_new$move),]
detects <- rbind(detects, shed_detects)
detects$mov_count <- NULL
detects$move <- NULL

## Now export shed tag summaries
write.csv(shed_new,file = "shed_tags.csv", row.names = F)


#### Now summarize beacon detects by day ####

#beacon_by_day <- aggregate(detects_beacon1$DateTime_PST, by = list(recv = detects_beacon1$recv,location = detects_beacon1$location, TagCode = detects_beacon1$TagCode, day = format(detects_beacon1$DateTime_PST, format = "%Y-%m-%d")), FUN = length)
detects_beacon$day <- format(detects_beacon$DateTime_PST, format = "%Y-%m-%d")
beacon_by_day_new <- detects_beacon[,list(count = length(DateTime_PST)), by = list(recv,location,TagCode,day)]
## remove temp_detects now that aggregate operation is over
detects_beacon$day <- NULL

beacon_by_day_new$day <- as.Date(beacon_by_day_new$day)

## now merge with all the beacon_by_day records, making sure to overwrite the ones from the last ~30 days

## First, find the records in beacon_by_day that are also in beacon_by_day_new
remove <- match(do.call("paste", beacon_by_day_new[, c("location", "recv", "TagCode", "day")]), do.call("paste", beacon_by_day[, c("location", "recv", "TagCode", "day")]))

## Now remove these records from beacon_by_day
beacon_by_day <- beacon_by_day[-remove[is.na(remove)==F],]

## Make sure beacon_by_day$day stays as a date format, this can get stripped when beacon_by_day is reduced to zero rows
beacon_by_day$day <- as.Date(beacon_by_day$day)

## Now merge the two datasets together
beacon_by_day <- rbindlist(list(beacon_by_day, beacon_by_day_new))

## In this step, associate the correct beacon to the correct recv at that given time, this allows for inseason changes
beacon_by_day$beacon <- as.character(NA)

for (i in 1:nrow(beacon_deployments)){
  beacon_by_day[which(beacon_deployments$recv[i] == beacon_by_day$recv & beacon_deployments$start[i] <= beacon_by_day$day & (beacon_deployments$stop[i] > beacon_by_day$day | is.na(beacon_deployments$stop[i]))),"beacon"] <- beacon_deployments$beacon[i]
}
## now write this new beacon_by_day file
fwrite(beacon_by_day, "beacon_by_day.csv", row.names = F)

## Now remove days when there wasn't a full day of beacon detections yet
#beacon_by_day <- beacon_by_day[is.na(beacon_by_day$beacon)==F,]
## Now subset to only look at data for the correct beacon for that day
beacon_by_day <- beacon_by_day[which(beacon_by_day$TagCode == beacon_by_day$beacon),]
## Remove detections at receivers that didn't have beacons yet but detected other's beacons
#beacon_by_day <- beacon_by_day[-which(beacon_by_day$location == "GeorgSlRT_1.1" & beacon_by_day$day < as.Date("2018-06-19")),]
#beacon_by_day <- beacon_by_day[-which(beacon_by_day$recv %in% c(17136,17139,17140,17142) & beacon_by_day$day < as.Date("2018-06-19")),]
## now put in NAs for days when no beacons were deployed at receivers
beacon_by_day_add <- as.data.frame(rbind(cbind("BlGeorg_RT1.2", seq.Date(from = as.Date("2018-04-16"),to = as.Date("2018-06-18"), by = "day")), cbind("BlGeorg_RT1.1", seq.Date(from = as.Date("2018-04-16"),to = as.Date("2018-06-18"), by = "day")), cbind("GeorgSlRT_1.2", seq.Date(from = as.Date("2018-04-17"),to = as.Date("2018-06-18"), by = "day")), cbind("GeorgSlRT_1.1", seq.Date(from = as.Date("2018-04-17"),to = as.Date("2018-06-18"), by = "day"))), stringsAsFactors = F)
## Massage these records for binding with beacon_by_day
colnames(beacon_by_day_add) <- c("location", "day")
beacon_by_day_add$day <- as.Date(as.numeric(beacon_by_day_add$day), origin = "1970-01-01")
beacon_by_day <- bind_rows(beacon_by_day, beacon_by_day_add)
## Now bring in recv_start for next step
beacon_by_day <- merge(beacon_by_day, gap_percent[, c("location", "recv_start")], all.x = T)
## now remove detections that occured  before the official start time (see above note about receivers shutting off soon after deployment)
beacon_by_day <- beacon_by_day[which(beacon_by_day$day > beacon_by_day$recv_start),]

## now associate these beacon_by_day records with gap_percent
gap_percent <- merge(gap_percent, aggregate(list(day_count = beacon_by_day$day), by = list(location = beacon_by_day$location), FUN = length), by = "location", all.x = T)
## Estimate how many hours and days have elapsed
gap_percent$num_of_days <- round(difftime(Sys.time(), gap_percent$recv_start, units = "d"), 0)
## Now estimate the number of days with no beacon detections
gap_percent$percent_downtime <- 100-round(gap_percent$day_count/as.numeric(gap_percent$num_of_days)*100,1)

## Estimate daily detection efficiency
beacon_by_day <- merge(beacon_by_day,unique(beacon_deployments[,c("beacon", "beacon_PRI")]), by = "beacon", all.x = T)
beacon_by_day$expected <- 24*60*60/beacon_by_day$beacon_PRI
## now adjust detections for today to be representative of efficiency to this point in the day
beacon_by_day[as.Date(beacon_by_day$day) == as.Date(format(Sys.time(),tz="Etc/GMT+8")),"expected"] <- period_to_seconds(hms(format(floor_date(Sys.time()-60*60, unit = "hour"),tz="Etc/GMT+8", format = "%H:%M:%S")))/beacon_by_day[as.Date(beacon_by_day$day) == as.Date(format(Sys.time(),tz="Etc/GMT+8")),"beacon_PRI"]
## Now estimate efficiency, also make values over 100 impossible
beacon_by_day$day_efficiency <- round(beacon_by_day$count/beacon_by_day$expected,3)*100
beacon_by_day[which(beacon_by_day$day_efficiency >100), "day_efficiency"] <- 100

## Finally, make a table with all-time and last week's mean daiy detection efficiency
efficiency <- aggregate(list(all_time_efficiency = beacon_by_day$day_efficiency), by = list(location = beacon_by_day$location), FUN = mean, na.rm = T)
last_week <- beacon_by_day[beacon_by_day$day < as.Date(Sys.time()) & beacon_by_day$day > as.Date(Sys.time())-8,] 
efficiency <- merge(efficiency, aggregate(list(last_week_efficiency = last_week$day_efficiency), by = list(location = last_week$location), FUN = mean, na.rm = T), all.x = T)
efficiency$all_time_efficiency <- round(efficiency$all_time_efficiency,1)
efficiency$last_week_efficiency <- round(efficiency$last_week_efficiency,1)


detects <- detects[order(detects$general_location, detects$TagCode, detects$DateTime_PST),]

#### Now run detection filter per study ID, and save as seperate csvs, but also as one combined one for ERDDAP
study_detections <- NULL

for (k in unique(tagcodes$StudyID)){
  studycodes <- tagcodes[tagcodes$StudyID == k, "TagID_Hex"]
  ## First, select only detections of correct tagcodes per study
  temp_detects <- as.data.frame(detects[detects$TagCode %in% studycodes$TagID_Hex,])
  ## Now remove detections that occurred on 4/1/19 at 17135
  temp_detects <- temp_detects[!(temp_detects$recv == "17135" & as.Date(temp_detects$DateTime_PST) == "2019-04-01"),]
  ## Now remove false detections that occured before release,
  temp_detects <- merge(temp_detects, tagcodes[,c("TagID_Hex", "RelDT", "PRI_nominal", "tag_life")], by.x = "TagCode", by.y = "TagID_Hex")
  temp_detects <- temp_detects[temp_detects$DateTime_PST > temp_detects$RelDT,]
  ## Now remove false detections that occured after the tags expected end of life
  if(nrow(temp_detects) > 0){
    ## Remove detections that occur past 150% of tag life
    temp_detects$tag_end <- temp_detects$RelDT + (temp_detects$tag_life*24*60*60)*1.5
    temp_detects <- temp_detects[temp_detects$DateTime_PST < temp_detects$tag_end,]
  }
  # ## Currently, all data has filter of 3 detections within 10 minutes. But since last 15 minutes of every recv detection history is also kept, we might as well rerun the filter to remove these detections
  # if(nrow(temp_detects) > 0){
  #   ## Now create two columns with time in hours between the previous and next detection, for each detection
  #   ## Now reorder the data since the above steps may have jumbled things
  #   temp_detects <- temp_detects[order(temp_detects$TagCode, temp_detects$general_location, temp_detects$DateTime_PST),]
  #   ## Now estimate the time in secs between the previous and next detection, for each detection.
  #   temp_detects$time_to_next <- as.numeric(difftime(shift(temp_detects$DateTime_PST, fill = NA, type = "lead"), temp_detects$DateTime_PST, units = "secs"))
  #   temp_detects$time_from_previous <- as.numeric(difftime(temp_detects$DateTime_PST, shift(temp_detects$DateTime_PST, fill = NA, type = "lag"),  units = "secs"))
  #   ## Now make NA the time diff values when it's between 2 different gen locs or tagcodes
  #   temp_detects[which(temp_detects$general_location != shift(temp_detects$general_location, fill = NA, type = "lag")), "time_from_previous"] <- NA
  #   temp_detects[which(temp_detects$general_location != shift(temp_detects$general_location, fill = NA, type = "lead")), "time_to_next"] <- NA
  #   temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
  #   temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, fill = NA, type = "lead")), "time_to_next"] <- NA
  #   
  #   
  #   
  #   ## Now mark tag detections that have a detection both before and after from the same tag at the same gen loc within 10 minute span (this seems like a good cutoff) (within 5 minutes before and 5 minutes after).
  #   temp_detects$before_and_after_valid <- 0
  #   temp_detects[is.na(temp_detects$time_to_next) == F & is.na(temp_detects$time_from_previous) == F & temp_detects$time_to_next < (300) & temp_detects$time_from_previous < (300),"before_and_after_valid"] <- 1
  #   
  #   ## now determine unique gen loc visits based on the before_after_valid criteria. Two zeros in a row means a new visit
  #   temp_detects$prev_before_after_valid <- shift(temp_detects$before_and_after_valid, fill = NA, type = "lag")
  #   
  #   ## Create a counter that says whether to tick visit_num up by one
  #   temp_detects$visit_step <- 0
  #   temp_detects[which(temp_detects$before_and_after_valid == 0 & temp_detects$prev_before_after_valid == 0),"visit_step"] <- 1
  #   
  #   ## just do cumulative sum of the visit step to get visit num
  #   temp_detects$visit_num <- cumsum(temp_detects$visit_step)
  #   ## Now repeat estimation of time to next and time from previous, but setting visit_num changes to NA now
  #   temp_detects <- temp_detects[order(temp_detects$TagCode, temp_detects$general_location, temp_detects$DateTime_PST),]
  #   ## Now estimate the time in secs between the previous and next detection, for each detection.
  #   temp_detects$time_to_next <- as.numeric(difftime(shift(temp_detects$DateTime_PST, fill = NA, type = "lead"), temp_detects$DateTime_PST, units = "secs"))
  #   temp_detects$time_from_previous <- as.numeric(difftime(temp_detects$DateTime_PST, shift(temp_detects$DateTime_PST, fill = NA, type = "lag"),  units = "secs"))
  #   ## Now make NA the time diff values when it's between 2 different visits or tagcodes
  #   temp_detects[which(temp_detects$visit_num != shift(temp_detects$visit_num, fill = NA, type = "lag")), "time_from_previous"] <- NA
  #   temp_detects[which(temp_detects$visit_num != shift(temp_detects$visit_num, fill = NA, type = "lead")), "time_to_next"] <- NA
  #   
  #   
  #   ## now estimate the time elapsed between first and last detection of a visit in secs
  #   min_max_diff <- aggregate(list(minmaxdiff = temp_detects$DateTime_PST), by = list(temp_detects$TagCode, temp_detects$visit_num), function(x){as.numeric(difftime(max(x), min(x), units = "secs"))})
  #   
  #   ## Also include number of detections per visit
  #   min_max_diff <- merge(min_max_diff,aggregate(list(count = temp_detects$DateTime_PST), by = list(temp_detects$TagCode, temp_detects$visit_num), length))
  #   min_max_diff$ping_density <- min_max_diff$count / min_max_diff$minmaxdiff
  #   ## remove less than 3 pings per visits (these are detections from last 15 minutes that weren't removed in the original 10min window filter)
  #   min_max_diff <- min_max_diff[min_max_diff$count>2,]
  #   temp_detects <- temp_detects[temp_detects$visit_num %in% min_max_diff$Group.2,]
  # }
  # if(nrow(temp_detects) > 0){
  #   ## Now remove any visit that has a tag density of less than 4 pings per PRI*16.6 AND less than 5 detections total
  #   temp_detects <- temp_detects[!temp_detects$visit_num %in% min_max_diff[min_max_diff$ping_density < (4/(temp_detects$PRI_nominal*16.6)) & min_max_diff$count < 5, "Group.2"],]
  # }
  
  if(nrow(temp_detects) > 0){
    ## Now create two columns with time in hours between the previous and next detection, for each detection
    ## Now reorder the data since the above steps may have jumbled things
    temp_detects <- temp_detects[order(temp_detects$TagCode, temp_detects$recv, temp_detects$DateTime_PST),]
    ## Now estimate the time in secs between the previous and next detection, for each detection.
    temp_detects$time_to_next <- as.numeric(difftime(shift(temp_detects$DateTime_PST, fill = NA, type = "lead"), temp_detects$DateTime_PST, units = "secs"))
    temp_detects$time_from_previous <- as.numeric(difftime(temp_detects$DateTime_PST, shift(temp_detects$DateTime_PST, fill = NA, type = "lag"),  units = "secs"))
    temp_detects$next_interval <- as.numeric(difftime(shift(temp_detects$DateTime_PST, fill = NA, n = 2, type = "lead"), shift(temp_detects$DateTime_PST, fill = NA, n = 1, type = "lead"), units = "secs"))
    temp_detects$prev_interval <- as.numeric(difftime(shift(temp_detects$DateTime_PST, fill = NA, n = 1, type = "lag"), shift(temp_detects$DateTime_PST, fill = NA, n = 2, type = "lag"), units = "secs"))
    
    ## Now make NA the time diff values when it's between 2 different recv or tagcodes
    temp_detects[which(temp_detects$recv != shift(temp_detects$recv, fill = NA, type = "lag")), "time_from_previous"] <- NA
    temp_detects[which(temp_detects$recv != shift(temp_detects$recv, fill = NA, type = "lead")), "time_to_next"] <- NA
    temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
    temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, fill = NA, type = "lead")), "time_to_next"] <- NA
    temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, n = 2, fill = NA, type = "lead")), "next_interval"] <- NA
    temp_detects[which(temp_detects$recv != shift(temp_detects$recv, fill = NA, n = 2, type = "lead")), "next_interval"] <- NA
    temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, n = 2, fill = NA, type = "lag")), "prev_interval"] <- NA
    temp_detects[which(temp_detects$recv != shift(temp_detects$recv, fill = NA, n = 2, type = "lag")), "prev_interval"] <- NA
    # Is current detection within PRI*1.3*12+1 seconds from the previous two, the one before and one after, and the following two?
    temp_detects$part_of_valid_3 <- rowSums(cbind(temp_detects$time_to_next + temp_detects$next_interval < (temp_detects$PRI_nominal * 1.3 * 12) +1,
                                                  temp_detects$time_to_next + temp_detects$time_from_previous < (temp_detects$PRI_nominal * 1.3 * 12) +1,
                                                  temp_detects$time_from_previous + temp_detects$prev_interval < (temp_detects$PRI_nominal * 1.3 * 12) +1), na.rm = T)
    ## Remove detects that are not part of at least one train of 3 valid detections
    temp_detects <- temp_detects[temp_detects$part_of_valid_3 >0,]
  }
  
  if(nrow(temp_detects) > 0){
    ## Now recalculate diff times
    ## reorder the data since the above steps may have jumbled things
    temp_detects <- temp_detects[order(temp_detects$TagCode, temp_detects$recv, temp_detects$DateTime_PST),]
    ## Now estimate the time in secs between the previous and next detection, for each detection.
    temp_detects$time_to_next <- as.numeric(difftime(shift(temp_detects$DateTime_PST, fill = NA, type = "lead"), temp_detects$DateTime_PST, units = "secs"))
    temp_detects$time_from_previous <- as.numeric(difftime(temp_detects$DateTime_PST, shift(temp_detects$DateTime_PST, fill = NA, type = "lag"),  units = "secs"))
    temp_detects$next_interval <- as.numeric(difftime(shift(temp_detects$DateTime_PST, fill = NA, n = 2, type = "lead"), shift(temp_detects$DateTime_PST, fill = NA, n = 1, type = "lead"), units = "secs"))
    temp_detects$prev_interval <- as.numeric(difftime(shift(temp_detects$DateTime_PST, fill = NA, n = 1, type = "lag"), shift(temp_detects$DateTime_PST, fill = NA, n = 2, type = "lag"), units = "secs"))
    
    ## Now make NA the time diff values when it's between 2 different gen locs or tagcodes
    temp_detects[which(temp_detects$recv != shift(temp_detects$recv, fill = NA, type = "lag")), "time_from_previous"] <- NA
    temp_detects[which(temp_detects$recv != shift(temp_detects$recv, fill = NA, type = "lead")), "time_to_next"] <- NA
    temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
    temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, fill = NA, type = "lead")), "time_to_next"] <- NA
    temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, n = 2, fill = NA, type = "lead")), "next_interval"] <- NA
    temp_detects[which(temp_detects$recv != shift(temp_detects$recv, fill = NA, n = 2, type = "lead")), "next_interval"] <- NA
    temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, n = 2, fill = NA, type = "lag")), "prev_interval"] <- NA
    temp_detects[which(temp_detects$recv != shift(temp_detects$recv, fill = NA, n = 2, type = "lag")), "prev_interval"] <- NA
    
    ## Create a counter that says whether to tick visit_num up by one. These need to be when a recv or TagCode changes, OR when a fish revisits a receiver
    temp_detects$visit_step <- 0
    temp_detects[is.na(temp_detects$time_from_previous),"visit_step"] <- 1  #scenario 1. I've kept this here even though scenario 2 actually captures these as well
    temp_detects$first_of_three_good <- ((temp_detects$time_to_next + temp_detects$next_interval) < (temp_detects$PRI_nominal * 1.3 * 12) +1) ## setting up scenario 2
    temp_detects[temp_detects$part_of_valid_3 == 1 & is.na(temp_detects$first_of_three_good) == F & temp_detects$first_of_three_good == T,"visit_step"] <- 1  #scenario 2
    
    ## just do cumulative sum of the visit step to get visit num
    temp_detects$visit_num <- cumsum(temp_detects$visit_step)
    
    ## Now to calculate PRI, we'll need to recalculate time to next and time from previous once we've reordered by visit_num
    temp_detects <- temp_detects[order(temp_detects$visit_num, temp_detects$DateTime_PST),]
    ## Now estimate the time in secs between the previous and next detection, for each detection.
    temp_detects$time_to_next <- as.numeric(difftime(shift(temp_detects$DateTime_PST, fill = NA, type = "lead"), temp_detects$DateTime_PST, units = "secs"))
    temp_detects$time_from_previous <- as.numeric(difftime(temp_detects$DateTime_PST, shift(temp_detects$DateTime_PST, fill = NA, type = "lag"),  units = "secs"))
    temp_detects$time_to_2ndnext <- as.numeric(difftime(shift(temp_detects$DateTime_PST, fill = NA, n = 2, type = "lead"), temp_detects$DateTime_PST, units = "secs"))
    temp_detects$time_from_2ndprevious <- as.numeric(difftime(temp_detects$DateTime_PST, shift(temp_detects$DateTime_PST, fill = NA, n=2, type = "lag"),  units = "secs"))
    ## Now make NA the time diff values when it's between 2 different visit_nums
    temp_detects[which(temp_detects$visit_num != shift(temp_detects$visit_num, fill = NA, type = "lag")), "time_from_previous"] <- NA
    temp_detects[which(temp_detects$visit_num != shift(temp_detects$visit_num, fill = NA, type = "lead")), "time_to_next"] <- NA
    temp_detects[which(temp_detects$visit_num != shift(temp_detects$visit_num, n = 2, fill = NA, type = "lag")), "time_from_2ndprevious"] <- NA
    temp_detects[which(temp_detects$visit_num != shift(temp_detects$visit_num, n = 2, fill = NA, type = "lead")), "time_to_2ndnext"] <- NA
    
    ## function for estimating most likely PRI based on gap between detections
    estPRI <- function(span = span, PRI = PRI){
      ifelse((span > PRI & is.na(span) == F),(span/PRI) / round(span/PRI, digits = 0) * PRI,span)
    }
    
    ## If a middle detection from a ping train, look at PRI to before and after
    temp_detects$PRI.1 <- estPRI(span = temp_detects$time_from_previous, PRI = temp_detects$PRI_nominal)
    temp_detects$PRI.2 <- estPRI(span = temp_detects$time_to_next, PRI = temp_detects$PRI_nominal)
    ## If first detection from a ping train, look at 2 next subsequent PRIs
    temp_detects[is.na(temp_detects$PRI.1)==T,"PRI.1"] <- estPRI(span = temp_detects[is.na(temp_detects$PRI.1)==T,"time_to_2ndnext"], PRI = temp_detects[is.na(temp_detects$PRI.1)==T,"PRI_nominal"])
    ## If last detection from a ping train, look at 2 previous PRIs
    temp_detects[is.na(temp_detects$PRI.2)==T,"PRI.2"] <- estPRI(span = temp_detects[is.na(temp_detects$PRI.2)==T,"time_from_2ndprevious"], PRI = temp_detects[is.na(temp_detects$PRI.2)==T,"PRI_nominal"])
    ## Are the 2 PRIs good (within 20% of nominal)?
    temp_detects$two_good_PRIS <- apply(apply(temp_detects[,c("PRI.1", "PRI.2")], MARGIN = 2, function(x){abs(temp_detects$PRI_nominal - x) < temp_detects$PRI_nominal*0.20}), MARGIN = 1, FUN = sum)
    ## SD of 2 PRIs less than 0.025?
    temp_detects$PRI.sd <- apply(temp_detects[,c("PRI.1", "PRI.2")], MARGIN = 1, FUN = sd) < 0.025
    temp_detects <- temp_detects[which(temp_detects$two_good_PRIS > 0 & temp_detects$PRI.sd == T),]
  }
  
  ## Now remove less than 3 detects per visit
  if(nrow(temp_detects) > 0){  
    detects_per_visit_num <- aggregate(temp_detects$DateTime_PST, by = list(visit_num = temp_detects$visit_num), FUN = length)
    
    temp_detects <- temp_detects[temp_detects$visit_num %in% detects_per_visit_num[detects_per_visit_num$x > 2, "visit_num"],]
  }
  
  
  
  ## Now remove RelDT, PRI, tag_life, genloctag, and other unnecessary columns
  temp_detects <- temp_detects[, names(temp_detects) %in% c("TagCode" ,"location","recv","DateTime_Orig", "DateTime_PST", "Temp", "general_location", "latitude", "longitude", "rkm")]
  
  ## Add this data to study_detections file, only if there is something to add
  if(nrow(temp_detects) > 0){
    study_detections <- rbindlist(list(study_detections, cbind(k,temp_detects)))
  }
  temp_detects$DateTime_PST <- format(temp_detects$DateTime_PST, "%Y-%m-%d %H:%M:%S")
  fwrite(temp_detects, paste(getwd(),"/Study_detection_files/", paste("detects",k,sep = "_"),".csv", sep = ""), row.names = F)
}


#### Now export study_detections
## But first change column name for studyid
colnames(study_detections)[1] <- "Study_ID"

## Now merge in important release data
study_detections <- merge(study_detections, tagcodes, by.x = c("Study_ID", "TagCode"), by.y = c("StudyID", "TagID_Hex"), all.y = T)

study_detections$DateTime_PST <- format(study_detections$DateTime_PST, "%Y-%m-%d %H:%M:%S")
fwrite(study_detections[, c("Study_ID", "TagCode", "DateTime_PST", "recv", "location", "general_location", "latitude", "longitude", "rkm", "RelDT", "Weight", "Length", "Rel_rkm", "Rel_loc")], "study_detections.csv", row.names = F)


latest <- as.character(round.POSIXt(max(downloads$end, na.rm = T), units = "hour"))

## Now that all data manipulations are complete, export detects with milliseconds shown (this turns the class into character)
detects$DateTime_PST <- format(detects$DateTime_PST, "%Y-%m-%d %H:%M:%OS6")
detects_beacon$DateTime_PST <- format(detects_beacon$DateTime_PST, "%Y-%m-%d %H:%M:%OS6")

write.csv(latest, "latest_download.csv", row.names = F)
fwrite(downloads, "downloads.csv", row.names = F)
fwrite(detects, "detects_final.csv", row.names = F)
fwrite(detects_beacon, "detects_beacon.csv", row.names = F)
#write.csv(detects_filtered, "detects_filtered.csv", row.names = F)
#write.csv(gaps, "download_gaps.csv", row.names = F)

## Now delete the corrupted files that have been redownloaded <3 times, with hopes that they will be better when redownloaded, and remove them from ATS_processed_files so script knows to reprocess them
remove_these <- corrupted_downloads[corrupted_downloads$download_attempts <5,"filename"]
file.remove(remove_these)

write.csv(corrupted_downloads, "corrupted_downloads.csv", row.names = F)

ATS_processed_files <- rbindlist(list(ATS_processed_files, ATS_files[names(ATS_processed_files)]))

ATS_processed_files <- as.data.frame(list(x=ATS_processed_files[!ATS_processed_files$x %in% remove_these,]), stringsAsFactors = F )

fwrite(ATS_processed_files, "ATS_processed_files.csv", row.names = F)
TECKNO_processed_files <- rbindlist(list(TECKNO_processed_files, TECKNO_files))
TECKNO_processed_files$x <- as.character(TECKNO_processed_files$x)
fwrite(TECKNO_processed_files, "TECKNO_processed_files.csv", row.names = F, quote = T)  

UCD_processed_files <- rbindlist(list(UCD_processed_files, UCD_files))
UCD_processed_files$x <- as.character(UCD_processed_files$x)
fwrite(UCD_processed_files, "UCD_processed_files.csv", row.names = F, quote = T) 

```
***Data current as of `r anicon::nia(latest, size = 1, colour = "red")`. Updates occur hourly. All times in Pacific Standard Time.***

<br/>

## Map of Real-time Receiver Sites

```{r make map of realtime sites, echo=FALSE, warning=FALSE}

library(leaflet)
library(maps)
library(htmlwidgets)
library(leaflet.extras)

leaflet(data = gen_locs[is.na(gen_locs$stop),]) %>%
    # setView(-72.14600, 43.82977, zoom = 8) %>% 
    addProviderTiles("Esri.WorldStreetMap", group = "Map") %>%
    addProviderTiles("Esri.WorldImagery", group = "Satellite") %>% 
    addProviderTiles("Esri.WorldShadedRelief", group = "Relief") %>%
    # Marker data are from the sites data frame. We need the ~ symbols
    # to indicate the columns of the data frame.
    addMarkers(~longitude, ~latitude, label = ~general_location, group = "Receiver Sites", popup = ~location) %>% 
    # addAwesomeMarkers(~lon_dd, ~lat_dd, label = ~locality, group = "Sites", icon=icons) %>%
    addScaleBar(position = "bottomleft") %>%
    addLayersControl(
        baseGroups = c("Street Map", "Satellite", "Relief"),
        overlayGroups = c("Receiver Sites"),
        options = layersControlOptions(collapsed = FALSE)) %>%
    addSearchFeatures(targetGroups = c("Receiver Sites"))

```


<br/>

## Receiver sites
``` {r print site stats, echo=FALSE, warning=FALSE}

gen_locs <- gen_locs[order(gen_locs$rkm, decreasing = T),]

kable(gen_locs[is.na(gen_locs$stop),c("recv", "location", "general_location", "latitude", "longitude", "rkm")], align = "c", row.names = F)


````

<br/>

## Most recent downloads
``` {r print most recent downloads, echo=FALSE, warning=FALSE}

if (nrow(downloads_new_all) > 0) {
  for (i in 1:nrow(gen_locs)){
  downloads_new_all[which(gen_locs$recv[i] == downloads_new_all$recv & gen_locs$start[i] <= downloads_new_all$start & (gen_locs$stop[i] > downloads_new_all$end | is.na(gen_locs$stop[i]))),"location"] <- gen_locs[i,"location"]
  }

  downloads_new_all <- downloads_new_all[,c("recv", "start", "end", "location",
                                            "sun_volts", "bat_volts")]
  kable(downloads_new_all, row.names = F)
} else {
  "No new downloads"
}

````


<br/>

## Operational time for each real-time receiver since 1/1/2019

<br/>

```{r plot alltime receiver beacon detections, echo=FALSE, warning=FALSE, message=FALSE, fig.height = 10, fig.width = 10}

## Now make plots of receiver beacon detections through time. 
beacon_by_day <- merge(beacon_by_day, unique(gen_locs[,c("location", "rkm")]), by = "location")
beacon_by_day <- beacon_by_day[order(beacon_by_day$rkm, decreasing = F),]
beacon_by_day$location <- factor(beacon_by_day$location, unique(beacon_by_day$location))

beacon_by_day <- beacon_by_day[beacon_by_day$day > as.Date("2018-12-31"),]
par(mar=c(5.1,2.1,2.1,7.1))
plot(beacon_by_day$day, beacon_by_day$location, yaxt = "n", xlab = "Total time receiver has been deployed and operational since 1/1/2019", pch = 15)
axis(4, labels = unique(beacon_by_day$location), at = 1:length(unique(beacon_by_day$location)), las = 2, cex.axis = 0.7)
points(as.Date(aggregate(beacon_by_day$recv_start, by = list(beacon_by_day$location), FUN = min, na.rm = T)$x), unique(beacon_by_day$location), col = "red", pch = 15)


```

<br/>

## Detection filters applied to real-time detections

<br/>

Note: The detection filters used for filtering out false detections described below are fairly strict. The reasoning is that for the purposes of estimating survival, incorrectly keeping false detections can bias survival high, while incorrectly dropping real detections can bias detection probability low. In the latter case, under most circumstances, the survival estimate will remain mostly unchanged, although error will likely be higher. We have refined the detection filters to minimize the number of false detections that are included, as well as minimize the number of real detections that get dropped, but in situations of high uncertainty, we have taken the stricter approach and designed the filter to drop those detections for the reason stated above.

Filtering protocol:

1. “Multipath detection removal”: All detections that occur within 0.3 sec after an initial tag detection on the same receiver, that share the same Tag Code, will be removed from the data. These detections are "multipath detections" and are created by echos from the original tag transmission.


2. "3 detections within 6 minutes rule": As a first cut to remove obviously false detections, only detections of "potentially valid fish detections" are kept in the database. Potentially valid fish detections are when at least 3 consecutive detections of a tag code occur at a general location site with no more than 3 minute gaps in between the three detections. These potentially valid detections are kept in the general database, but stricter criteria need to be met for them to be associated with a released fish (see below).


3. “Is there a fish at large with that TagCode?”: For detections of a TagCode to be assigned to an actual released fish with that same TagCode, the detections need to fall between the release time of that fish, and the release time + 150% of the estimated tag life for that fish's tag. For example, if a fish is released on 1/1/2019, and it has a 60 day battery life, detections of that fish’s TagCode must fall between 1/1/2019 and 4/1/2019 to be associated to that fish.


4. “Detection density filter”: Once potentially valid fish detections have been determined and assigned to a specific fish, any 3 consecutive tag detections at an individual receiver must occur in a time window shorter than (Pulse Rate Interval (PRI) x 1.3 x 12)+ 1 seconds.  For example, for a 10 sec PRI, if a detection does not have at least 2 other detections within 157 seconds of it, it is removed. The total consecutive detections that fulfill this condition are considered a potentially valid fish visit.


5. “PRI filter”: the PRI from the previous detection and the PRI to the next detection on an individual receiver is estimated for each detection of a potentially valid fish visit. At least one of the PRIs must be within 20% of the tag’s programmed PRI AND the standard deviation of the two PRIs must be smaller than 0.025. If these two conditions are not both met, the detection is dropped. If the detection is the first of a receiver visit, then the PRIs to the subsequent two detections are used. If the detection is the last of a receiver visit, the PRIs from the previous two detections are used. 


6. “3-hit filter”: Once all the previous filters have been applied, if there are less than 3 detections total for a potentially valid fish visit, those visits are dropped. All remaining fish detections are kept and considered valid.


Additionally, a shed tag algorithm is used to remove detections for any tag that is detected continuously at one general location site for more than 30 days, and that was detected at that site a minimum of 20,000 times. The first 100 and last 100 detections of the general location site are kept for survival analyses, but the remaining detections are dropped.


<br/>

## All-time Detection Efficiency from Beacon Tags (when operational)

<br/>

```{r print table of detection efficiency}

last_download <- aggregate(list(last_download = downloads$end), by = list(location = downloads$location), FUN = max)

gen_loc_stats <- merge(unique(gen_locs[,c("location", "rkm")]), last_download, by = "location", all.x = T)

gen_loc_stats <- gen_loc_stats[order(gen_loc_stats$rkm, decreasing = T),]

gen_loc_stats <- merge(gen_loc_stats, gap_percent[c("location", "recv_start", "percent_downtime")], by = "location", all.x = T)

gen_loc_stats <- merge(gen_loc_stats, efficiency, by = "location", all.x= T)

gen_loc_stats <- gen_loc_stats[order(gen_loc_stats$rkm, decreasing = T),]

kable(gen_loc_stats[,c("location",  "recv_start", "last_week_efficiency", "all_time_efficiency", "last_download")], align = "c", row.names = F)


```

<br/>


## Daily Detection Efficiency from Beacon Tags over last 30 days

<br/>

```{r print heatmap of detection efficiency}


library(ggplot2)


## plot just last months worth of daily efficiency
#last_month <- beacon_by_day[which(as.POSIXct(beacon_by_day$day, format = "%Y-%m-%d") > Sys.time()-60*60*24*30),]
last_month <- beacon_by_day[which(beacon_by_day$day > as.Date(Sys.time()-60*60*24*30)),]

last_month$recv_lab <- paste(last_month$recv, last_month$location, sep = " ")
## Change order of data to plot decreasing rkm
last_month <- last_month[order(last_month$rkm, decreasing = F),]
last_month$location <- factor(last_month$location, unique(last_month$location))
## Now plot a heatmap of percent fish distribution per region, per timestep


ggplot() +
  geom_tile(data=last_month, aes(x=factor(day), y=location, fill=day_efficiency, colour = "")) + 
  scale_fill_gradient(low = "red", high = "blue", name = "% Daily Beacon Detection Efficiency", na.value = "black", limits=c(0, 100)) +
  scale_colour_manual(values = NA) +
  scale_y_discrete(position = "right") +
  scale_x_discrete(labels = 29:0) +
  labs(x="# of Days Ago", y = NULL) +
  theme(legend.position="bottom", panel.background = element_blank()) +
  guides(colour = guide_legend("No beacon tag", override.aes = list(colour = "black")))

```


<br/>


## Voltage for each ATS receiver

<br/>

```{r print voltage for ATS receivers,fig.height = 30, echo=FALSE, warning=FALSE, message=FALSE}

library(ggplot2)

volt.files <- data.frame(path =dir(pattern = "*.txt", path = "C:/Users/field/Documents/Voltage_emails", full.names = T), stringsAsFactors = F)
volt.files$Datetime <- as.POSIXct(substr(volt.files$path, 41, stop = 55), format = "%Y%m%d-%H%M%S", tz = "Etc/GMT+8" )

## Only use last 2 weeks of volt files
volt.files_2weeks <- volt.files[volt.files$Datetime > (Sys.time() - 60*60*24*14),]

IPs <- read.csv("C:/Users/field/Desktop/Real-time data massaging/products/IP_addresses.csv", stringsAsFactors = F)
volt.files_2weeks$IP <- NULL
volt.files_2weeks$Volts <- NULL


if (nrow(volt.files_2weeks) >0){
  for (i in 1:nrow(volt.files_2weeks)){
    test <- read.csv(volt.files_2weeks[i,"path"], skip = 3, stringsAsFactors = F)
    volt.files_2weeks[i,"Volts"] <- as.numeric(substr(test[1,],11,15))
    volt.files_2weeks[i,"IP"] <- substr(test[2,],5,nchar(test[2,]))
  }
  volt.files_2weeks <- merge(volt.files_2weeks, IPs[,c("RECV", "TCP_IP")], by.x = "IP", by.y = "TCP_IP")
  
  volt.files_2weeks <- volt.files_2weeks[,c("Datetime", "Volts", "RECV")]
} else{
  volt.files_2weeks <- data.frame("Datetime" = as.character(), "Volts" = numeric(), "RECV" = character(), stringsAsFactors = F)
  volt.files_2weeks$Datetime <- as.POSIXct(volt.files_2weeks$Datetime)
}

## Get last 2 weeks of Teknos
volt.teknos <- downloads[downloads$recv %in% c(157002,157003,157004,157005) & downloads$end > (Sys.time() - 60*60*24*14), c("end", "bat_volts", "recv") ]

colnames(volt.teknos) <- colnames(volt.files_2weeks)

volt.files_2weeks <- rbind(volt.files_2weeks, volt.teknos)
volt.files_2weeks$Volts <- as.numeric(volt.files_2weeks$Volts)

ggplot(volt.files_2weeks, aes(x = Datetime, y = Volts)) + 
  geom_line() + 
  geom_point() +
  ylim(9,15) +
  scale_x_datetime(date_breaks = "2 days", date_labels = "%Y-%m-%d") +
  geom_hline(yintercept=12, linetype="dashed", color = "red") +
  facet_grid(RECV ~ .) + 
  theme(legend.position = "none")

```


<br/>

```{r see winter run counts per tagger, echo=FALSE}

# setwd(paste(file.path(Sys.getenv("USERPROFILE"),"Desktop",fsep="\\"), "\\Real-time data massaging\\products", sep = ""))
# 
# 
# detects <- read.csv("detects_Winter_H_2018.csv", stringsAsFactors = FALSE)
# tagging <- read.csv("Tagged_Fish_Metadata.txt", stringsAsFactors = F)
# tagging <- tagging[tagging$StudyID == "Winter_H_2018",]
# 
# detects_tagger <- merge(detects, tagging[,c("TagID_Hex","Surgeon", "Rel_group")], by.x = "TagCode", by.y = "TagID_Hex")
# 
# arrivals <- as.data.frame(table(detects_tagger[!duplicated(detects_tagger$TagCode),"Surgeon"]))
# 
# colnames(arrivals) <- c("Surgeon", "Unique_Fish_Arrivals")
# arrivals$total_tagged <- 200
# arrivals$percent_arrivals <- round(arrivals$Unique_Fish_Arrivals/arrivals$total_tagged * 100,2)
# 
# kable(arrivals)


```


