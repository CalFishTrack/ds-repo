---
title: |
  | Central Valley Enhanced
  | Acoustic Tagging Project
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)

```

```{r logos, echo=FALSE, cache=TRUE}
htmltools::img(src = knitr::image_uri("../data/logos.jpg"), 
               alt = 'logo', 
               style = 'position:absolute; top:10px; right:0px; width:200px;')
```

<br/>


# *Real-time tracking Diagnostics*


```{r bring in data, massage, create new data files, print diagnostics, echo=FALSE, warning=FALSE, message=FALSE, results = "hide"}

library(knitr)
library(dplyr)
library(data.table)
library(lubridate)

dirs <- list.dirs(path= "C:/Advanced Telemetry Systems, Inc/ATS Trident Receiver/Data", recursive = F)
setwd(paste(file.path(Sys.getenv("USERPROFILE"),"Desktop",fsep="\\"), "\\Real-time data massaging\\products", sep = ""))

ATS_processed_files <- fread("ATS_processed_files.csv", stringsAsFactors = F)
TECKNO_processed_files <- fread("TECKNO_processed_files.csv", stringsAsFactors = F)
gen_locs <- read.csv("realtime_locs.csv", stringsAsFactors = F)
gen_locs$recv <- as.character(gen_locs$recv)
gen_locs$start <- as.POSIXct(gen_locs$start, tz = "Etc/GMT+8")
gen_locs$stop <- as.POSIXct(gen_locs$stop, tz = "Etc/GMT+8")
beacon_deployments <- read.csv("beacon_deployments.csv", stringsAsFactors = F)
beacon_deployments$start <- as.POSIXct(beacon_deployments$start, tz = "Etc/GMT+8")
beacon_deployments$stop <- as.POSIXct(beacon_deployments$stop, tz = "Etc/GMT+8")
beacon_deployments$recv <- as.character(beacon_deployments$recv)

beacon_by_day <- fread("beacon_by_day.csv", stringsAsFactors = F)

downloads <- read.csv("downloads.csv", stringsAsFactors = FALSE)
downloads$end <- ymd_hms(downloads$end, tz = "Etc/GMT+8")
downloads$start <- ymd_hms(downloads$start, tz = "Etc/GMT+8")
## remove all gen loc details from downloads, that way these get re-assigned every hour in case gen_loc data changes. This gets reassigned at the end of this code before exporting
downloads <- downloads[,-which(names(downloads) %in% c("location", "general_location", "latitude", "longitude", "rkm"))]

corrupted_downloads <- read.csv("corrupted_downloads.csv", stringsAsFactors = F)
tagcodes <- fread("qry_HexCodes.txt", stringsAsFactors = F)
tagcodes$RelDT <- as.POSIXct(tagcodes$RelDT, format = "%m/%d/%Y %I:%M:%S %p", tz = "Etc/GMT+8")

detects <- fread("detects_final.csv", stringsAsFactors = FALSE)
detects_beacon <- fread("detects_beacon.csv", stringsAsFactors = FALSE)

#detects$DateTime_PST <- as.POSIXct(detects$DateTime_PST, format = "%Y-%m-%d %H:%M:%OS ", tz = "Etc/GMT+8")
#detects_beacon$DateTime_PST <- as.POSIXct(detects_beacon$DateTime_PST, format = "%Y-%m-%d %H:%M:%OS ", tz = "Etc/GMT+8")
detects$DateTime_PST <- ymd_hms(detects$DateTime_PST, tz = "Etc/GMT+8")
detects_beacon$DateTime_PST <- ymd_hms(detects_beacon$DateTime_PST, tz = "Etc/GMT+8")

## At this point, let's remove any detects_beacon data that is older than 5 weeks and stick it in a weekly detects_beacon file. This will keep the amount of detections that gets uploaded into this script low so as to keep it running efficiently
## first, upload the time of last weekly data dump, and run this operation if it hasn't happened in a week
weekly_dump <- read.csv("weekly_data_dump.csv")
weekly_dump <- as.POSIXct(weekly_dump$x, format = "%Y-%m-%d %H:%M:%S")
if(difftime(Sys.time(),weekly_dump, units = "days") > 7){
  ## Find time exactly 5 weeks ago
  dump.end <- (Sys.time() - (35*24*60*60))
  ## Extract and export detections up to that date, making sure to save datetime as character to save milliseconds
  data_dump <- detects_beacon[detects_beacon$DateTime_PST < dump.end,]
  data_dump$DateTime_PST <- format(data_dump$DateTime_PST, "%Y-%m-%d %H:%M:%OS6")
  fwrite(data_dump, paste("C:/Users/Trailer/Desktop/Real-time data massaging/products/Detects_beacon_old/weekly_detects_beacon_ending_", as.Date(dump.end), ".csv", sep = ""), row.names = F)
  ## Remove those records from the detects_beacon dataframe
  detects_beacon <- detects_beacon[!detects_beacon$DateTime_PST < dump.end,]
  ## Now save current datetime if a dump occured
  write.csv(Sys.time(), "weekly_data_dump.csv", row.names = F)
}

## Now remove all gen loc data from detections in case genloc details have changed. We will add genloc data back in at the end of this code
detects <- subset(detects, select= -c(location, general_location, latitude, longitude, rkm))
detects_beacon <- subset(detects_beacon, select= -c(location, general_location, latitude, longitude, rkm))

## Find all Teckno file names
TECKNO_all_files <- as.data.frame(list(x=dir(pattern = "*.txt", path = "C:/Users/Trailer/Desktop/data", full.names = T)), stringsAsFactors = F)
## Select only newest TECKNO files for massaging
TECKNO_files <- as.data.frame(list(x=TECKNO_all_files[!TECKNO_all_files$x %in% TECKNO_processed_files$x,]), stringsAsFactors = F)

## Remove bad files that don't contain real data from list for processing
bad_files <- read.csv("C:/Users/Trailer/Desktop/Real-time data massaging/products/bad_files.csv", colClasses = "character", header = F)
TECKNO_files <- as.data.frame(list(x=TECKNO_files[-grep(paste(bad_files$V1, collapse = "|"), TECKNO_files$x),]), stringsAsFactors = F)


## Find all ATS file names
ATS_all_files <- as.data.frame(list(x=dir(pattern = ".csv", path = dirs, full.names = T)), stringsAsFactors = F)
## Select only newest ATS files for massaging
ATS_files <- as.data.frame(list(x=ATS_all_files[!ATS_all_files$x %in% ATS_processed_files$x,]), stringsAsFactors = F)
## Remove bad files that don't contain real data from list for processing
ATS_files <- as.data.frame(list(x=ATS_files[-grep(paste(bad_files$V1, collapse = "|"), ATS_files$x),]), stringsAsFactors = F)
## Load CSt timezone file list
CST_files <- read.csv("C:/Users/Trailer/Desktop/Real-time data massaging/products/receiver_files_with_CST_timezones.csv", colClasses = "character")

#### Data massage ATS files first ####

downloads_new_all <- data.frame()

if (nrow(ATS_files) > 0) {
  
  ## First, find shortname for ATS files
  shortname <- regexpr(ATS_files$x, pattern = "Data")[1]
  incomplete_shortname <- substr(ATS_files$x, shortname+5, nchar(ATS_files$x))
  shortname <- strsplit(incomplete_shortname, "/")
  ATS_files$shortname <- unlist(lapply(shortname, tail, 1))
  
  downloads_new1 <- data.frame()
  
  
  for (i in 1:nrow(ATS_files)) {
    if (file.info(ATS_files[i,"x"])$size >0){    
      if (substr(read.csv(ATS_files[i,"x"], nrows = 1, header = F)[,1] , 1,4) == "Site") {
        hour <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, skip = 7, nrows = 1, header = F)
        recv <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, skip = 2, nrows = 1, header = F)
      }else{
        recv <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, skip = 6, nrows = 1, header = F)
        hour <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, skip = 19, nrows = 1, header = F)
      }
      hour <- substr(hour, start = 13, stop = 31)
      recv <- substr(recv, start = 16, stop = 21)
      downloads_new1 <- rbindlist(list(downloads_new1, 
                                       data.frame(filename = ATS_files[i, "shortname"],recv, hour, stringsAsFactors = F)))#, use.names = F, fill = F, idcol = "filename")
    }
  }
  
  downloads_new1$start <- as.POSIXct(downloads_new1$hour, format = "%m/%d/%Y %H:%M:%S", tz = "Etc/GMT+8")
  
  ## Now change downloads start and end to correct time since some are on CST and some are PST + 1 day
  
  for (i in 1:nrow(downloads_new1)){ 
    if(downloads_new1[i, "filename"] %in% CST_files$Filenames) {
      downloads_new1[i, "start"] <- downloads_new1[i, "start"]-2*60*60
    }
  }
  
  downloads_new1$end <- downloads_new1$start +(60*60)
  
  ## Add dummy columns to match the ones from Tecknos that actually carry data
  downloads_new1$sun_volts <- NA
  downloads_new1$bat_volts <- NA
  
  ## Here, remove files that didn't have their download data read correctly and add them to the corrupted files list. These files should not be read for detections
  short_files <- ATS_files[ATS_files$shortname %in% downloads_new1[downloads_new1$recv == "","filename"],]
  if(nrow(short_files)){  
    downloads_new1 <- downloads_new1[!downloads_new1$recv == "",]
    ATS_files <- ATS_files[!ATS_files$shortname %in% short_files$shortname,]
    for (i in 1:nrow(short_files)){
      if(short_files[i, "shortname"] %in% corrupted_downloads$V1){
        corrupted_downloads[corrupted_downloads$V1 == short_files[i, "shortname"], "download_attempts"] <- 
          as.numeric(corrupted_downloads[corrupted_downloads$V1 == short_files[i, "shortname"], "download_attempts"])+1
      }else{
        corrupted_downloads <- rbind(corrupted_downloads, cbind(V1 = short_files[i, "shortname"], download_attempts = 1, filename = short_files[i,"x"]))
      }
    }
  }
  
  ## Now gather detections
  detects_temp <- data.frame()
  
  for (i in 1:nrow(ATS_files)) {
    ## First, put in error catch for files that are completely empty
    
    if (file.info(ATS_files[i,"x"])$size >0){
      if (substr(read.csv(ATS_files[i,"x"], nrows = 1, header = F)[,1] , 1,4) == "Site") {
        test <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, skip = 9, header = T,  row.names=NULL)
      }else{
        test <- read.csv(ATS_files[i,"x"], stringsAsFactors = F, skip = 22, header = T,  row.names=NULL)
      }
      
      if(colnames(test)[1] != "Internal"){
        colnames(test) <- colnames(test)[-1]
        test[,ncol(test)] <- NULL
      }
      ## Find file end, if it exists (if it doesn't, file is corrupted, see below)
      end1 <- head(grep("--", test[,1]),1)
      end2 <- head(grep("File End", test[,1]),1)
      end_final <- head(grep(".-- Done", test[,1]),1)
    }
    
    ## This is another error catcher for weird ATS files that are corrupted
    trytest <- try(as.POSIXct(test[is.na(test$DateTime)==F,"DateTime"], format = "%m/%d/%Y %H:%M:%OS", tz = "Etc/GMT+8"), silent = T)
    
    if (length(end_final) == 0 | file.info(ATS_files[i,"x"])$size == 0 | inherits(trytest, "try-error") | nrow(test[is.na(test$DateTime)==F,]) == 0) {
      ## This means the detection file is incomplete aka corrupted
      ## Or, the second term above tests if the file is completely empty, aka also corrupted
      ## If the file is corrupted and new, add to corrupted list
      ## if the file is corrupted and a repeat, tally 1 to the counter
      ## At the end of this code, Remove the file from the downloads table
      ## And physically remove the file from the computer if the tally counter is <5
      ## If >= 5, leave the file as this will avoid having to download it again to no avail
      ## We do this at the end so we don't delete files when prototyping this ATS code segment
      downloads_new1 <- downloads_new1[!downloads_new1$filename == ATS_files[i,"shortname"],]
      if(ATS_files[i, "shortname"] %in% corrupted_downloads$V1){
        corrupted_downloads[corrupted_downloads$V1 == ATS_files[i, "shortname"], "download_attempts"] <- 
          as.numeric(corrupted_downloads[corrupted_downloads$V1 == ATS_files[i, "shortname"], "download_attempts"])+1
      }else{
        corrupted_downloads <- rbind(corrupted_downloads, cbind(V1 = ATS_files[i, "shortname"], download_attempts = 1, filename = ATS_files[i,"x"]))
      }
      
    }else{
      
      ## if a previously corrupted file did finally get read correctly, remove it from the corrupted file
      if(ATS_files[i, "shortname"] %in% corrupted_downloads$V1){
        corrupted_downloads <- corrupted_downloads[!corrupted_downloads$V1 == ATS_files[i, "shortname"],] 
      }
      ## Now data massage
      test$recv <- substr(ATS_files[i,"x"], nchar(ATS_files[i,"x"])-22, nchar(ATS_files[i,"x"])-18)
      test1 <- test[-c(end_final:nrow(test)),]
      test2 <- test1[!test1$TagCode == "",]
      test3 <- test2[!test2$TagCode == " TagCode",]
      
      test4 <- test3[!test3$TagCode == " GPS Fix  ",]
      
      names(test4)[names(test4) == 'DateTime'] <- 'DateTime_Orig'
      
      test4$DateTime_Orig <- as.POSIXct(test4$DateTime_Orig, format = "%m/%d/%Y %H:%M:%OS", tz = "Etc/GMT+8")
      
      test4$DateTime_PST <- test4$DateTime_Orig
      
      ## Now change DateTime_PST to PST, but this is tricky because for some, orig time is in CST
      ## Have this first if statement in case there are no detections in a file
      if(nrow(test4)>0){
        if(ATS_files[i,"x"] %in% CST_files$Filenames) {
          test4$DateTime_PST <- test4$DateTime_PST-2*60*60
          attributes(test4$DateTime_PST)$tzone <- "Etc/GMT+8"
        }
        
        ## Finally, if erroneous receiver serial number "09000"" comes up, change it to 18092
        test4[test4$recv == "09000", "recv"] <- "18092"
        
        ## change Datetime_Orig with milliseconds shown (this turns the class into character)
        test4$DateTime_Orig <- format(test4$DateTime_Orig, "%Y-%m-%d %H:%M:%OS6")
        
        detects_temp <- rbindlist(list(detects_temp, test4))
        
        
      }
    }
  }
  
  
  downloads_new_all <- rbindlist(list(downloads_new_all, downloads_new1[,c("filename", "recv", "hour", "sun_volts", "bat_volts", "end", "start")]))
  
  #locs <- detects_new[detects_new$TagCode == " GPS Fix  ",]
  #N <- unlist(gregexpr(pattern ='N',locs$Internal))
  #locs$lat <- as.numeric(substr(locs$Internal, start = 1, stop = N-1))
  #locs$lon <- as.numeric(substr(locs$Internal, start = N+1, stop= nchar(locs$Internal)-2))
  #locs$lat <- round(locs$lat, 0)/100
  #locs$lon <- round(locs$lon, 0)/100*-1
  
  #coords <- unique(locs[,c("recv", "lat", "lon")])
  
  ## In case all new files were corrupted i.e. length(detects_temp) = 0, have this if statement
  if (length(detects_temp) > 0){
    ## Now remove prefix and suffix of TagCode
    detects_temp$TagCode <- substr(detects_temp$TagCode, start = 5, stop = 8)
    
    ## Remove "ffff" detections
    detects_temp <- detects_temp[!detects_temp$TagCode == "ffff",]
    
    ## Make temp numeric
    detects_temp$Temp <- as.numeric(detects_temp$Temp)
    
    ## Make sure critical rows are NA, this has been happening on rare occasions
    detects_temp <- detects_temp[!(is.na(detects_temp$recv) | is.na(detects_temp$DateTime_PST) | is.na(detects_temp$TagCode)),]
    
    ## Sometimes weird negative or 99.99 degree temp recordings are logged, remove these
    detects_temp[detects_temp$Temp > 99 | detects_temp$Temp < 0, "Temp"] <- NA
    
    ## A few receivers seem to have had temp sensors, so make NA
    #detects_temp[detects_temp$recv %in% c(17127, 17139, 17143, 17147, 18008, 18003, 17144, 18002, 18006, 18007, 18092), "Temp"] <- NA
    
    ## Here, put only non-beacon detections in detection file
    detects_new <- detects_temp[!detects_temp$TagCode %in% c(unique(beacon_deployments$beacon), "006F", "006D"),]
    ## And put beacon detections into seperate file
    detects_beacon_new <- detects_temp[detects_temp$TagCode %in% c(unique(beacon_deployments$beacon), "006F", "006D"),]
    
    ## Now remove multipath detections from detects_beacon_new
    ## First reorder the data since the above steps may have jumbled things
    detects_beacon_new <- detects_beacon_new[order(detects_beacon_new$recv, detects_beacon_new$TagCode, detects_beacon_new$DateTime_PST),]
    detects_beacon_new$time_to_next <- as.numeric(difftime(shift(detects_beacon_new$DateTime_PST, fill = NA, type = "lead"), detects_beacon_new$DateTime_PST, units = "hours"))
    detects_beacon_new$time_from_previous <- as.numeric(difftime(detects_beacon_new$DateTime_PST, shift(detects_beacon_new$DateTime_PST, fill = NA, type = "lag"),  units = "hours"))
    ## Now make NA the time diff values when it's between 2 different recvs or tagcodes
    detects_beacon_new[which(detects_beacon_new$recv != shift(detects_beacon_new$recv, fill = NA, type = "lag")), "time_from_previous"] <- NA
    detects_beacon_new[which(detects_beacon_new$recv != shift(detects_beacon_new$recv, fill = NA, type = "lead")), "time_to_next"] <- NA
    detects_beacon_new[which(detects_beacon_new$TagCode != shift(detects_beacon_new$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
    detects_beacon_new[which(detects_beacon_new$TagCode != shift(detects_beacon_new$TagCode, fill = NA, type = "lead")), "time_to_next"] <- NA
    ## Now remove the second detection every time the detection lag difftime is less than 1 sec (multipath)
    detects_beacon_new <- detects_beacon_new[detects_beacon_new$time_from_previous > 0.00028 | is.na(detects_beacon_new$time_from_previous) == TRUE ,]
    
    #detects_new <- merge(detects_new, coords, by = "recv", all.x = T)
    
    #detects_new <- merge(detects_new, gen_locs, by = "recv", all.x = T)
    
    detects_beacon <- rbindlist(list(detects_beacon, detects_beacon_new[,c("recv", "DateTime_Orig", "TagCode", "DateTime_PST", "Temp")]))
    detects <- rbindlist(list(detects, detects_new[,c("recv", "DateTime_Orig", "TagCode", "DateTime_PST", "Temp")]))
  }  
}




#######################################
#### Now data massage TECKNO files ####
#######################################




if (nrow(TECKNO_files) > 0) {
  downloads_new2 <- data.frame()
  for (i in 1:nrow(TECKNO_files)) {
    test <- try(read.csv(TECKNO_files[i,], header = F, stringsAsFactors = F ), silent = T) 
    if (inherits(test, "try-error") | ncol(test) < 18) {
      next
    } else {
      test1 <- tail(test[nchar(test$V5) > 4,],1)
      hour <- test1$V3
      recv <- test1$V1
      ## Get volts, sometimes in row 1 or 2
      sun_volts <- max(test[1:2,"V17"])
      bat_volts <- max(test[1:2,"V18"])
      downloads_new2 <- rbindlist(list(downloads_new2, data.frame(list(filename = TECKNO_files[i,], recv=recv, hour=hour, sun_volts = sun_volts, bat_volts = bat_volts), stringsAsFactors = F)))
    }
  }
  
  downloads_new2$end <- as.POSIXct(downloads_new2$hour, tz = "GMT")
  downloads_new2$start <- downloads_new2$end -(60*60)
  
  
  
  ## Now change all times to PST to match ATS receivers
  attributes(downloads_new2$start)$tzone <- "Etc/GMT+8"
  attributes(downloads_new2$end)$tzone <- "Etc/GMT+8"
  
  downloads_new_all <- rbindlist(list(downloads_new_all, downloads_new2))
  
  detects_temp <- data.frame()
  
  ## Put in progress bar
  #pb <- txtProgressBar(min = 0, max = nrow(TECKNO_files), style = 3)
  
  for (i in 1:nrow(TECKNO_files)) {
    res <- try(read.csv(TECKNO_files[i,], header = F), silent = T) 
    if (inherits(res, "try-error")) {
      corrupted <- strsplit(TECKNO_files[i,], "/")
      corrupted2 <- unlist(lapply(corrupted, tail, 1))
      corrupted_downloads <- rbind(corrupted_downloads,cbind(V1 = corrupted2, download_attempts = 3, filename = TECKNO_files[i,]))
      ## Also make corrupted downloads more palateable
      next
    } else {
      start <- nchar(read.csv(TECKNO_files[i,],stringsAsFactors = F, header = F)$V5)
      start <- head(which(start==4),1)
      if (length(start) > 0) {
        test <- read.csv(TECKNO_files[i,], skip = start-1, header = F, stringsAsFactors = F )
        test1 <- test[nchar(test$V5)==4,]
        detects_temp <- rbindlist(list(detects_temp, test1[,1:9]))
        
      }    
    }
    #setTxtProgressBar(pb, i)
  }
  #close(pb)
  
  colnames(detects_temp) <- c("recv", "V2", "DateTime_Orig", "dec_seconds", "TagCode", "V6", "V7", "V8", "V9")
  detects_temp <- detects_temp[!detects_temp$V2 == "N",]
  ## In this step, append datetime to milliseconds, but make sure the 6 decimal places are represented for all cases, even when there are trailering zeros
  detects_temp$DateTime_Orig <- paste(detects_temp$DateTime_Orig, substr(formatC(as.numeric(detects_temp$dec_seconds),format='f', digits=6 ),2,8), sep = "")
  ## Make a POSIX class, note that TECKNO detections are in GMT, will need to convert to local time
  detects_temp$DateTime_PST <- as.POSIXct(detects_temp$DateTime_Orig, format = "%Y-%m-%d %H:%M:%OS", tz = "GMT")
  attributes(detects_temp$DateTime_PST)$tzone <- "Etc/GMT+8"
  
  ## Add column for temperature
  detects_temp$Temp <- NA
  
  ## Here, put only non-beacon detections in detection file
  detects_new <- detects_temp[!detects_temp$TagCode %in% c(unique(beacon_deployments$beacon), "006F", "006D"),]
  ## And put beacon detections into seperate file
  detects_beacon_new <- detects_temp[detects_temp$TagCode %in% c(unique(beacon_deployments$beacon), "006F", "006D"),]
  
  ## Now remove multipath detections from detects_beacon_new
  ## First reorder the data since the above steps may have jumbled things
  detects_beacon_new <- detects_beacon_new[order(detects_beacon_new$recv, detects_beacon_new$TagCode, detects_beacon_new$DateTime_PST),]
  detects_beacon_new$time_to_next <- as.numeric(difftime(shift(detects_beacon_new$DateTime_PST, fill = NA, type = "lead"), detects_beacon_new$DateTime_PST, units = "hours"))
  detects_beacon_new$time_from_previous <- as.numeric(difftime(detects_beacon_new$DateTime_PST, shift(detects_beacon_new$DateTime_PST, fill = NA, type = "lag"),  units = "hours"))
  ## Now make NA the time diff values when it's between 2 different recvs or tagcodes
  detects_beacon_new[which(detects_beacon_new$recv != shift(detects_beacon_new$recv, fill = NA, type = "lag")), "time_from_previous"] <- NA
  detects_beacon_new[which(detects_beacon_new$recv != shift(detects_beacon_new$recv, fill = NA, type = "lead")), "time_to_next"] <- NA
  detects_beacon_new[which(detects_beacon_new$TagCode != shift(detects_beacon_new$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
  detects_beacon_new[which(detects_beacon_new$TagCode != shift(detects_beacon_new$TagCode, fill = NA, type = "lead")), "time_to_next"] <- NA
  ## Now remove the second detection every time the detection lag difftime is less than 1 sec (multipath)
  detects_beacon_new <- detects_beacon_new[detects_beacon_new$time_from_previous > 0.00028 | is.na(detects_beacon_new$time_from_previous) == TRUE ,]
  
  ## Now append these to existing datasets
  detects <- rbindlist(list(detects, detects_new[,c("recv", "DateTime_Orig", "TagCode", "DateTime_PST", "Temp")]))
  detects_beacon <- rbindlist(list(detects_beacon, detects_beacon_new[,c("recv", "DateTime_Orig", "TagCode", "DateTime_PST", "Temp")]))
  
}




## Now remove duplicate detections that may have occured when an hourly file was inadvertantly uploaded twice
detects <- detects[!duplicated(detects[,c("recv", "DateTime_Orig", "TagCode")])]
detects_beacon <- detects_beacon[!duplicated(detects_beacon[,c("recv", "DateTime_Orig", "TagCode")])]

## Now merge location data back with detections and detects_beacon

## Now assign the location name and other location data based on the start and stop time in gen_locs
## Note that this will assign NA for all location data if a detection doesn't get assigned to a location
for (i in 1:nrow(gen_locs)){
  detects[which(gen_locs$recv[i] == detects$recv & gen_locs$start[i] <= detects$DateTime_PST & (gen_locs$stop[i] > detects$DateTime_PST | is.na(gen_locs$stop[i]))),c("location", "general_location", "latitude", "longitude", "rkm")] <- gen_locs[i, c("location", "general_location", "latitude", "longitude", "rkm")]
}

for (i in 1:nrow(gen_locs)){
  detects_beacon[which(gen_locs$recv[i] == detects_beacon$recv & gen_locs$start[i] <= detects_beacon$DateTime_PST & (gen_locs$stop[i] > detects_beacon$DateTime_PST | is.na(gen_locs$stop[i]))),c("location", "general_location", "latitude", "longitude", "rkm")] <- gen_locs[i, c("location", "general_location", "latitude", "longitude", "rkm")]
}

## Merge new downloads with exist dataset
downloads <- rbindlist(list(downloads, downloads_new_all))

## Merge in gen loc data now for all downloads
downloads[,c("location", "general_location", "latitude", "longitude", "rkm")] <- as.character(NA)

for (i in 1:nrow(gen_locs)){
  downloads[which(gen_locs$recv[i] == downloads$recv & gen_locs$start[i] <= downloads$start & (gen_locs$stop[i] > downloads$end | is.na(gen_locs$stop[i]))),c("location", "general_location", "latitude", "longitude", "rkm")] <- gen_locs[i,c("location", "general_location", "latitude", "longitude", "rkm")]
}

#gaps <- downloads[is.na(downloads$gapstart) == F,]
#gaps$gapstart <- as.POSIXct(gaps$gapstart,format = "%Y-%m-%d %H:%M:%S", tz = "Etc/GMT+8")
#gaps$gapend <- as.POSIXct(gaps$gapend, format = "%Y-%m-%d %H:%M:%S", tz = "Etc/GMT+8")

#gaps$gap_length_hours <- round(difftime(gaps$gapend, gaps$gapstart, units = "hours"), 0)
#gaps$start <- NULL
#gaps$end <- NULL

gap_percent <- aggregate(list(total_files = downloads$start), by = list(location = downloads$location), FUN = length) 
gap_percent <- merge(gap_percent,aggregate(list(recv_start = downloads$start), by = list(location = downloads$location), FUN = min), by = "location", all.y = T)
## Some receivers turned hourly recording off soon after deployment, meaning there is an initial gap in the data, so I'm setting the true start time to when it started uninterrupted recording (i.e. after the gap)
gap_percent[gap_percent$location == "I80-50_Br1", "recv_start"] <- as.POSIXct("2018-05-14 10:00:00")
gap_percent[gap_percent$location == "RT_OldRiver", "recv_start"] <- as.POSIXct("2018-04-18 15:00:00")
gap_percent[gap_percent$location == "GeorgSlRT_1.2", "recv_start"] <- as.POSIXct("2018-04-17 9:00:00")
gap_percent[gap_percent$location == "GeorgSlRT_1.1", "recv_start"] <- as.POSIXct("2018-04-17 4:00:00")
gap_percent[gap_percent$location == "BeniciaRT_14", "recv_start"] <- as.POSIXct("2018-04-24 18:00:00")
gap_percent[gap_percent$location == "BeniciaRT_02", "recv_start"] <- as.POSIXct("2018-04-25 17:00:00")
gap_percent[gap_percent$location == "DCCRT_1", "recv_start"] <- as.POSIXct("2018-05-16 12:00:00")
gap_percent[gap_percent$location == "GeorgSlRT_2.1", "recv_start"] <- as.POSIXct("2018-05-22 16:00:00")

## Calculate the individual hours that have beacon detections
## First, make sure only a receivers specific beacon tag is allowed to count
#detects_beacon1 <- merge(detects_beacon, gen_locs[,c("recv", "beacon")], by = "recv", all.x = T)
#detects_beacon1 <- detects_beacon

#detects_beacon1 <- detects_beacon1[detects_beacon1$TagCode == detects_beacon1$beacon,]
#beacon_by_hour <- aggregate(detects_beacon1$DateTime_PST, by = list(recv = detects_beacon1$recv,TagCode = detects_beacon1$TagCode, hour = format(detects_beacon1$DateTime_PST, format = "%Y-%m-%d %H")), FUN = length)
#beacon_by_hour <- merge(beacon_by_hour, gap_percent[, c("recv", "recv_start")])
#beacon_by_hour <- beacon_by_hour[beacon_by_hour$hour > beacon_by_hour$recv_start,]
#gap_percent <- merge(gap_percent, aggregate(list(hour_count = beacon_by_hour$hour), by = list(recv = beacon_by_hour$recv), FUN = length), by = "recv", all.x = T)


## Now remove multipaths from detects_final
## Now create two columns with time in hours between the previous and next detection, for each detection
## first order data appropriately

detects <- detects[order(detects$recv, detects$TagCode, detects$DateTime_PST),]
detects$time_from_previous <- as.numeric(difftime(detects$DateTime_PST, shift(detects$DateTime_PST, fill = NA, type = "lag"),  units = "secs"))
## Now make NA the time diff values when it's between 2 different gen locs or tagcodes
detects[which(detects$recv != shift(detects$recv, fill = NA, type = "lag")), "time_from_previous"] <- NA
detects[which(detects$TagCode != shift(detects$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
## Now remove the second detection every time the detection lag difftime is less than 1 sec (multipath and funny behavior)
detects <- detects[detects$time_from_previous > 5 | is.na(detects$time_from_previous) == TRUE ,]
detects$time_from_previous <- NULL

## Now remove detections that don't meet the filtering criteria

## Now create two columns with time in hours between the previous and next detection, for each detection
## Now reorder the data since the above steps may have jumbled things
detects <- detects[order(detects$TagCode, detects$DateTime_PST),]
## Now estimate the time in hours between the previous and next detection, for each detection. 
detects$time_to_next <- as.numeric(difftime(shift(detects$DateTime_PST, fill = NA, type = "lead"), detects$DateTime_PST, units = "hours"))
detects$time_from_previous <- as.numeric(difftime(detects$DateTime_PST, shift(detects$DateTime_PST, fill = NA, type = "lag"),  units = "hours"))
## Now make NA the time diff values when it's between 2 different gen locs or tagcodes
detects[which(detects$general_location != shift(detects$general_location, fill = NA, type = "lag")), "time_from_previous"] <- NA
detects[which(detects$general_location != shift(detects$general_location, fill = NA, type = "lead")), "time_to_next"] <- NA
detects[which(detects$TagCode != shift(detects$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
detects[which(detects$TagCode != shift(detects$TagCode, fill = NA, type = "lead")), "time_to_next"] <- NA

## Now mark tag detections that have a detection both before and after from the same tag at the same gen loc within 10 minute span (this seems like a good cutoff) (within 5 minutes before and 5 minutes after). We call this "filter_lag", that way it can be adjusted per studyID when necessary (in our case, it has been for green sturgeon)
detects$before_and_after_valid <- 0
detects[is.na(detects$time_to_next) == F & is.na(detects$time_from_previous) == F & detects$time_to_next < 0.08333333 & detects$time_from_previous < 0.08333333,"before_and_after_valid"] <- 1
## In case a valid fish detection occurs right before the hour mark, we need to make sure to keep these, otherwise they might get dropped before the remaining detections come in the next hour
last_detects <- aggregate(list(last = detects$DateTime_PST), by = list(location = detects$location), FUN = max)
last_detects$last <-  last_detects$last - 15*60
detects <- merge(detects, last_detects, by = "location")
## Now pull out only these detections that are part of at least 3 consecutive detections that are no more than 5 minutes apart
## Also keep detections that occured within the last 15 minutes of each recv so that valid tag visits aren't dropped before all the detections come in
detects <- detects[which(shift(detects$before_and_after_valid, fill = NA, type = "lag") == 1 |
                                     shift(detects$before_and_after_valid, fill = NA, type = "lead") == 1 |
                                     detects$before_and_after_valid == 1 |
                                     detects$DateTime_PST > detects$last),]
## Remove extra columns
detects$last <- NULL
detects$time_to_next <- NULL
detects$time_from_previous <- NULL
detects$before_and_after_valid <- NULL

## Now summarize beacon detects by day
#beacon_by_day <- aggregate(detects_beacon1$DateTime_PST, by = list(recv = detects_beacon1$recv,location = detects_beacon1$location, TagCode = detects_beacon1$TagCode, day = format(detects_beacon1$DateTime_PST, format = "%Y-%m-%d")), FUN = length)
detects_beacon$day <- format(detects_beacon$DateTime_PST, format = "%Y-%m-%d")
beacon_by_day_new <- detects_beacon[,list(count = length(DateTime_PST)), by = list(recv,location,TagCode,day)]
## remove temp_detects now that aggregate operation is over
detects_beacon$day <- NULL

beacon_by_day_new$day <- as.Date(beacon_by_day_new$day)

## now merge with all the beacon_by_day records, making sure to overwrite the ones from the last ~30 days

## First, find the records in beacon_by_day that are also in beacon_by_day_new
remove <- match(do.call("paste", beacon_by_day_new[, c("location", "recv", "TagCode", "day")]), do.call("paste", beacon_by_day[, c("location", "recv", "TagCode", "day")]))

## Now remove these records from beacon_by_day
beacon_by_day <- beacon_by_day[-remove[is.na(remove)==F],]

## Make sure beacon_by_day$day stays as a date format, this can get stripped when beacon_by_day is reduced to zero rows
beacon_by_day$day <- as.Date(beacon_by_day$day)

## Now merge the two datasets together
beacon_by_day <- rbindlist(list(beacon_by_day, beacon_by_day_new))

## now write this new beacon_by_day file
fwrite(beacon_by_day, "beacon_by_day.csv", row.names = F)

## In this step, associate the correct beacon to the correct recv at that given time, this allows for inseason changes
beacon_by_day$beacon <- as.character(NA)

for (i in 1:nrow(beacon_deployments)){
  beacon_by_day[which(beacon_deployments$recv[i] == beacon_by_day$recv & beacon_deployments$start[i] <= beacon_by_day$day & (beacon_deployments$stop[i] > beacon_by_day$day | is.na(beacon_deployments$stop[i]))),"beacon"] <- beacon_deployments$beacon[i]
}
## Now remove days when there wasn't a full day of beacon detections yet
#beacon_by_day <- beacon_by_day[is.na(beacon_by_day$beacon)==F,]
## Now subset to only look at data for the correct beacon for that day
beacon_by_day <- beacon_by_day[which(beacon_by_day$TagCode == beacon_by_day$beacon),]
## Remove detections at receivers that didn't have beacons yet but detected other's beacons
#beacon_by_day <- beacon_by_day[-which(beacon_by_day$location == "GeorgSlRT_1.1" & beacon_by_day$day < as.Date("2018-06-19")),]
#beacon_by_day <- beacon_by_day[-which(beacon_by_day$recv %in% c(17136,17139,17140,17142) & beacon_by_day$day < as.Date("2018-06-19")),]
## now put in NAs for days when no beacons were deployed at receivers
beacon_by_day_add <- as.data.frame(rbind(cbind("BlGeorg_RT1.2", seq.Date(from = as.Date("2018-04-16"),to = as.Date("2018-06-18"), by = "day")), cbind("BlGeorg_RT1.1", seq.Date(from = as.Date("2018-04-16"),to = as.Date("2018-06-18"), by = "day")), cbind("GeorgSlRT_1.2", seq.Date(from = as.Date("2018-04-17"),to = as.Date("2018-06-18"), by = "day")), cbind("GeorgSlRT_1.1", seq.Date(from = as.Date("2018-04-17"),to = as.Date("2018-06-18"), by = "day"))), stringsAsFactors = F)
## Massage these records for binding with beacon_by_day
colnames(beacon_by_day_add) <- c("location", "day")
beacon_by_day_add$day <- as.Date(as.numeric(beacon_by_day_add$day), origin = "1970-01-01")
beacon_by_day <- bind_rows(beacon_by_day, beacon_by_day_add)
## Now bring in recv_start for next step
beacon_by_day <- merge(beacon_by_day, gap_percent[, c("location", "recv_start")], all.x = T)
## now remove detections that occured  before the official start time (see above note about receivers shutting off soon after deployment)
beacon_by_day <- beacon_by_day[which(beacon_by_day$day > beacon_by_day$recv_start),]

## now associate these beacon_by_day records with gap_percent
gap_percent <- merge(gap_percent, aggregate(list(day_count = beacon_by_day$day), by = list(location = beacon_by_day$location), FUN = length), by = "location", all.x = T)
## Estimate how many hours and days have elapsed
gap_percent$num_of_days <- round(difftime(Sys.time(), gap_percent$recv_start, units = "d"), 0)
## Now estimate the number of days with no beacon detections
gap_percent$percent_downtime <- 100-round(gap_percent$day_count/as.numeric(gap_percent$num_of_days)*100,1)
## Now for receivers with no beacon, or a broken beacon, just use the old method of number of hourly files compared to number expected
#gap_percent[gap_percent$recv %in% c("17131", "17136", "17139", "17140", "17142"), "percent_downtime"] <- 100-round(gap_percent[gap_percent$recv %in% c("17131", "17136", "17139", "17140", "17142"), "total_files"]/as.numeric(gap_percent[gap_percent$recv %in% c("17131", "17136", "17139", "17140", "17142"), "num_of_hours"])*100,1)

## remove the receivers without beacons THESE NOW HAVE BEACON TAGS aS OF 6/19/18, THIS SHOULD NO LONGER BE NECESSARY
#beacon_by_hour <- beacon_by_hour[!beacon_by_hour$recv %in% c("17131", "17136", "17139", "17140", "17142"),]
#beacon_by_hour$recv <- factor(beacon_by_hour$recv)

## Estimate hourly detection efficiency
#beacon_by_hour <- merge(beacon_by_hour,gen_locs[,c("recv", "beacon_PRI")], by = "recv", all.x = T)
#beacon_by_hour$expected <- 60*60/beacon_by_hour$beacon_PRI
#beacon_by_hour$hourly_efficiency <- round(beacon_by_hour$x/beacon_by_hour$expected,3)*100

## Estimate daily detection efficiency
beacon_by_day <- merge(beacon_by_day,unique(beacon_deployments[,c("beacon", "beacon_PRI")]), by = "beacon", all.x = T)
beacon_by_day$expected <- 24*60*60/beacon_by_day$beacon_PRI
## now adjust detections for today to be representative of efficiency to this point in the day
beacon_by_day[as.Date(beacon_by_day$day) == as.Date(format(Sys.time(),tz="Etc/GMT+8")),"expected"] <- period_to_seconds(hms(format(floor_date(Sys.time()-60*60, unit = "hour"),tz="Etc/GMT+8", format = "%H:%M:%S")))/beacon_by_day[as.Date(beacon_by_day$day) == as.Date(format(Sys.time(),tz="Etc/GMT+8")),"beacon_PRI"]
## Now estimate efficiency, also make values over 100 impossible
beacon_by_day$day_efficiency <- round(beacon_by_day$count/beacon_by_day$expected,3)*100
beacon_by_day[which(beacon_by_day$day_efficiency >100), "day_efficiency"] <- 100

## Finally, make a table with all-time and last week's mean daiy detection efficiency
efficiency <- aggregate(list(all_time_efficiency = beacon_by_day$day_efficiency), by = list(location = beacon_by_day$location), FUN = mean, na.rm = T)
last_week <- beacon_by_day[beacon_by_day$day < as.Date(Sys.time()) & beacon_by_day$day > as.Date(Sys.time())-8,] 
efficiency <- merge(efficiency, aggregate(list(last_week_efficiency = last_week$day_efficiency), by = list(location = last_week$location), FUN = mean, na.rm = T), all.x = T)
efficiency$all_time_efficiency <- round(efficiency$all_time_efficiency,1)
efficiency$last_week_efficiency <- round(efficiency$last_week_efficiency,1)




#downloads$gapstart <- NULL
#downloads$gapend <- NULL

## Find data gaps by finding hours for which there are zero detectsion
## These should never happen since there are so many beacon detections

# res <- aggregate(detects_beacon$TagCode,
#                  by=list(detects_beacon$recv, format(detects_beacon$DateTime_PST, "%Y-%m-%d %H")),
#                  length)
# 
# colnames(res) <- c("recv", "DateTimeHour_PST", "detect_count")
# 
# hours <- as.data.frame(format(seq(
#   from= min(detects_beacon$DateTime_PST)+60*60*24,
#   to=as.POSIXct(Sys.time(), tz="Etc/GMT+8")-60*60*12,
#   by="hour"
# ),
# "%Y-%m-%d %H"))
# 
# colnames(hours) <- "DateTimeHour_PST"
# 
# res2 <- dcast(data = res, DateTimeHour_PST ~ recv)
# 
# res3 <- merge(hours, res2, by = "DateTimeHour_PST", all.x = T)
# 
# gaps <- res3[rowSums(is.na(res3)) > 0,]
# 
# gaps <- melt(gaps)
# 
# gaps <- gaps[is.na(gaps$value)==T,]
# 
# gaps$value <- NULL
# 
# colnames(gaps) <- c("DateHour_PST", "recv")


#### As a preliminary cut to detection filter, a tag needs to have been detected at least 3 times in the entire array
## NB: not worth doing any more, all tag code options have happened at least 3 times


#tags <- sort(table(detects$TagCode), decreasing = T)
#tags <- tags[tags>2]
#detects_filtered <- detects[detects$TagCode %in% rownames(tags),]
detects <- detects[order(detects$general_location, detects$TagCode, detects$DateTime_PST),]

#### Now run detection filter per study ID, and save as seperate csvs, but also as one combined one for ERDDAP
study_detections <- NULL

for (k in unique(tagcodes$StudyID)){
  studycodes <- tagcodes[tagcodes$StudyID == k, "TagID_Hex"]
  ## First, select only detections of correct tagcodes per study
  temp_detects <- as.data.frame(detects[detects$TagCode %in% studycodes$TagID_Hex,])
  ## Now remove false detections that occured before release,
  temp_detects <- merge(temp_detects, tagcodes[,c("TagID_Hex", "RelDT", "PRI_nominal")], by.x = "TagCode", by.y = "TagID_Hex")
  temp_detects <- temp_detects[temp_detects$DateTime_PST > temp_detects$RelDT,]
  ## Now remove false detections that occured after the tags expected end of life
  temp_detects$tag_life <- NA
  temp_detects[temp_detects$PRI_nominal %in% c(3,5), "tag_life"] <- 30
  temp_detects[temp_detects$PRI_nominal > 9 & temp_detects$PRI_nominal < 11, "tag_life"] <- 60
  ## Set tag life for sturgeon tags to 150. Also, while at it, set filter_lag for the allowable amount of time between detections, in this case, its 5 minutes for all chinook runs, and 10 minutes for green sturgeon
  if (k %in% c("Juv_Green_Sturgeon_2017", "Juv_Green_Sturgeon_2018")){
    temp_detects$tag_life <- 150
  }
  ## Remove detections that occur past 150% of tag life
  temp_detects$tag_end <- temp_detects$RelDT + (temp_detects$tag_life*24*60*60)*1.5
  temp_detects <- temp_detects[temp_detects$DateTime_PST < temp_detects$tag_end,]
  
  ## Currently, all data has filter of 3 detections within 10 minutes. But since last 15 minutes of every recv detection history is also kept, we might as well rerun the filter to remove these detections
  if(nrow(temp_detects) > 0){
   ## Now create two columns with time in hours between the previous and next detection, for each detection
   ## Now reorder the data since the above steps may have jumbled things
   temp_detects <- temp_detects[order(temp_detects$TagCode, temp_detects$DateTime_PST),]
   ## Now estimate the time in hours between the previous and next detection, for each detection.
   temp_detects$time_to_next <- as.numeric(difftime(shift(temp_detects$DateTime_PST, fill = NA, type = "lead"), temp_detects$DateTime_PST, units = "hours"))
   temp_detects$time_from_previous <- as.numeric(difftime(temp_detects$DateTime_PST, shift(temp_detects$DateTime_PST, fill = NA, type = "lag"),  units = "hours"))
   ## Now make NA the time diff values when it's between 2 different gen locs or tagcodes
   temp_detects[which(temp_detects$general_location != shift(temp_detects$general_location, fill = NA, type = "lag")), "time_from_previous"] <- NA
   temp_detects[which(temp_detects$general_location != shift(temp_detects$general_location, fill = NA, type = "lead")), "time_to_next"] <- NA
   temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, fill = NA, type = "lag")), "time_from_previous"] <- NA
   temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, fill = NA, type = "lead")), "time_to_next"] <- NA
  

   ## Now mark tag detections that have a detection both before and after from the same tag at the same gen loc within 10 minute span (this seems like a good cutoff) (within 5 minutes before and 5 minutes after).
   temp_detects$before_and_after_valid <- 0
   temp_detects[is.na(temp_detects$time_to_next) == F & is.na(temp_detects$time_from_previous) == F & temp_detects$time_to_next < 0.08333333 & temp_detects$time_from_previous < 0.08333333,"before_and_after_valid"] <- 1
   ## Now pull out only these detections that are part of at least 3 consecutive detections that are no more than 14 minutes apart
   temp_detects <- temp_detects[which(shift(temp_detects$before_and_after_valid, fill = NA, type = "lag") == 1 |
                                          shift(temp_detects$before_and_after_valid, fill = NA, type = "lead") == 1 |
                                          temp_detects$before_and_after_valid == 1),]
   #temp_detects[which(temp_detects$TagCode != shift(temp_detects$TagCode, fill = NA, type = "lead")), "time_to_next"] <- NA
   ## Remove before and after valid column
   temp_detects <- temp_detects[, !names(temp_detects) == "before_and_after_valid"]
  }
  
  ## Now remove RelDT, PRI, tag_life and genloctag columns
  temp_detects <- temp_detects[, !names(temp_detects) %in% c("RelDT", "genloctag", "PRI_nominal", "tag_life", "tag_end")]
  ## Add this data to study_detections file, only if there is something to add
  if(nrow(temp_detects) > 0){
      study_detections <- rbindlist(list(study_detections, cbind(k,temp_detects)))
  }
  temp_detects$DateTime_PST <- format(temp_detects$DateTime_PST, "%Y-%m-%d %H:%M:%S")
  fwrite(temp_detects, paste(paste("detects",k,sep = "_"),".csv", sep = ""), row.names = F)
}

#### Now export study_detections
## But first change column name for studyid
colnames(study_detections)[1] <- "Study_ID"
study_detections$DateTime_PST <- format(study_detections$DateTime_PST, "%Y-%m-%d %H:%M:%S")
fwrite(study_detections[, c("Study_ID", "TagCode", "DateTime_PST", "general_location", "latitude", "longitude", "rkm")], "study_detections.csv", row.names = F)


latest <- as.character(round.POSIXt(max(downloads$end), units = "hour"))

## Now that all data manipulations are complete, export detects with milliseconds shown (this turns the class into character)
detects$DateTime_PST <- format(detects$DateTime_PST, "%Y-%m-%d %H:%M:%OS6")
detects_beacon$DateTime_PST <- format(detects_beacon$DateTime_PST, "%Y-%m-%d %H:%M:%OS6")

write.csv(latest, "latest_download.csv", row.names = F)
fwrite(downloads, "downloads.csv", row.names = F)
fwrite(detects, "detects_final.csv", row.names = F)
fwrite(detects_beacon, "detects_beacon.csv", row.names = F)
#write.csv(detects_filtered, "detects_filtered.csv", row.names = F)
#write.csv(gaps, "download_gaps.csv", row.names = F)

## Now delete the corrupted files that have been redownloaded <3 times, with hopes that they will be better when redownloaded, and remove them from ATS_processed_files so script knows to reprocess them
remove_these <- corrupted_downloads[corrupted_downloads$download_attempts <5,"filename"]
file.remove(remove_these)

write.csv(corrupted_downloads, "corrupted_downloads.csv", row.names = F)

ATS_processed_files <- rbindlist(list(ATS_processed_files, ATS_files[names(ATS_processed_files)]))

ATS_processed_files <- as.data.frame(list(x=ATS_processed_files[!ATS_processed_files$x %in% remove_these,]), stringsAsFactors = F )

fwrite(ATS_processed_files, "ATS_processed_files.csv", row.names = F)
TECKNO_processed_files <- rbindlist(list(TECKNO_processed_files, TECKNO_files))
TECKNO_processed_files$x <- as.character(TECKNO_processed_files$x)
fwrite(TECKNO_processed_files, "TECKNO_processed_files.csv", row.names = F, quote = T)  



```
***Data current as of <span style="color:red">`r latest`</span>. All times in Pacific Standard Time.***

<br/>

## Map of Real-time Receiver Sites

```{r make map of realtime sites, echo=FALSE, warning=FALSE}

library(leaflet)
library(maps)
library(htmlwidgets)

leaflet(data = gen_locs[is.na(gen_locs$stop),]) %>%
    # setView(-72.14600, 43.82977, zoom = 8) %>% 
    addProviderTiles("Esri.WorldStreetMap", group = "Map") %>%
    addProviderTiles("Esri.WorldImagery", group = "Satellite") %>% 
    addProviderTiles("Esri.WorldShadedRelief", group = "Relief") %>%
    # Marker data are from the sites data frame. We need the ~ symbols
    # to indicate the columns of the data frame.
    addMarkers(~longitude, ~latitude, label = ~general_location, group = "Receiver Sites", popup = ~location) %>% 
    # addAwesomeMarkers(~lon_dd, ~lat_dd, label = ~locality, group = "Sites", icon=icons) %>%
    addScaleBar(position = "bottomleft") %>%
    addLayersControl(
        baseGroups = c("Street Map", "Satellite", "Relief"),
        overlayGroups = c("Receiver Sites"),
        options = layersControlOptions(collapsed = FALSE)
    )

```


<br/>

## Receiver sites
``` {r print site stats, echo=FALSE, warning=FALSE}

gen_locs <- gen_locs[order(gen_locs$rkm, decreasing = T),]

kable(gen_locs[is.na(gen_locs$stop),c("recv", "location", "general_location", "latitude", "longitude", "rkm")], align = "c", row.names = F)


````

<br/>

## Most recent downloads
``` {r print most recent downloads, echo=FALSE, warning=FALSE}

if (nrow(downloads_new_all) > 0) {
  for (i in 1:nrow(gen_locs)){
  downloads_new_all[which(gen_locs$recv[i] == downloads_new_all$recv & gen_locs$start[i] <= downloads_new_all$start & (gen_locs$stop[i] > downloads_new_all$end | is.na(gen_locs$stop[i]))),"location"] <- gen_locs[i,"location"]
  }

  downloads_new_all <- downloads_new_all[,c("recv", "start", "end", "location",
                                            "sun_volts", "bat_volts")]
  kable(downloads_new_all, row.names = F)
} else {
  "No new downloads"
}

````


<br/>

## Operational time for each receiver over entire study period

<br/>

Please note, receivers 17136, 17139, 17140, and 17142 did not have operational beacon tags until 6/19/2018

```{r plot alltime receiver beacon detections, echo=FALSE, warning=FALSE, message=FALSE, fig.height = 10, fig.width = 10}

## Now make plots of receiver beacon detections through time. 
beacon_by_day <- merge(beacon_by_day, unique(gen_locs[,c("location", "rkm")]), by = "location")
beacon_by_day <- beacon_by_day[order(beacon_by_day$rkm, decreasing = F),]
beacon_by_day$location <- factor(beacon_by_day$location, unique(beacon_by_day$location))

par(mar=c(5.1,2.1,2.1,7.1))
plot(beacon_by_day$day, beacon_by_day$location, yaxt = "n", xlab = "Total time receiver has been deployed and operational over whole study", pch = 15)
axis(4, labels = unique(beacon_by_day$location), at = 1:length(unique(beacon_by_day$location)), las = 2, cex.axis = 0.7)
points(as.Date(aggregate(beacon_by_day$recv_start, by = list(beacon_by_day$location), FUN = min, na.rm = T)$x), unique(beacon_by_day$location), col = "red", pch = 15)


```

<br/>

## Detection filters applied to real-time detections

<br/>

1. Multipath detection removal: All detections that occur within one second of the original tag detection on the same receiver, that share the same Tag Code, will be removed from the data. These detections are "multipath detections" and are created by echos from the original detection.

2. 3 detections within 10 minutes rule: In order to remove false detections, only detections of "valid fish visits" are kept in the database. A valid fish visit is when at least 3 consecutive detections of a tag code occur at a general location site with no more than 5 minute gaps in between the three detections.

3. Detections fall between release time and 150% of battery life: For detections to be assigned to an actual released fish, the detections need to fall between the release time of that fish, and the release time + 150% of the estimated tag life for that fish's tag.

<br/>

## Percent Down Time and Overall Detection Efficiency from Beacon Tags (when operational)

<br/>

```{r print table of detection efficiency}

last_download <- aggregate(list(last_download = downloads$end), by = list(location = downloads$location), FUN = max)

gen_loc_stats <- merge(unique(gen_locs[,c("location", "rkm")]), last_download, by = "location", all.x = T)

gen_loc_stats <- gen_loc_stats[order(gen_loc_stats$rkm, decreasing = T),]

gen_loc_stats <- merge(gen_loc_stats, gap_percent[c("location", "recv_start", "percent_downtime")], by = "location", all.x = T)

gen_loc_stats <- merge(gen_loc_stats, efficiency, by = "location", all.x= T)

gen_loc_stats <- gen_loc_stats[order(gen_loc_stats$rkm, decreasing = T),]

kable(gen_loc_stats[,c("location",  "recv_start", "percent_downtime", "last_week_efficiency", "all_time_efficiency", "last_download")], align = "c", row.names = F)


```

<br/>


## Detection Efficiency from Beacon Tags over last 30 days

<br/>

```{r print heatmap of detection efficiency}


library(ggplot2)

## plot just last months worth of daily efficiency
#last_month <- beacon_by_day[which(as.POSIXct(beacon_by_day$day, format = "%Y-%m-%d") > Sys.time()-60*60*24*30),]
last_month <- beacon_by_day[which(beacon_by_day$day > as.Date(Sys.time()-60*60*24*30)),]

last_month$recv_lab <- paste(last_month$recv, last_month$location, sep = " ")
## Change order of data to plot decreasing rkm
last_month <- last_month[order(last_month$rkm, decreasing = F),]
last_month$location <- factor(last_month$location, unique(last_month$location))
## Now plot a heatmap of percent fish distribution per region, per timestep


ggplot() +
  geom_tile(data=last_month, aes(x=factor(day), y=location, fill=day_efficiency, colour = "")) + 
  scale_fill_gradient(low = "red", high = "blue", name = "% Daily Beacon Detection Efficiency", na.value = "black") +
  scale_colour_manual(values = NA) +
  scale_y_discrete(position = "right") +
  scale_x_discrete(labels = 29:0) +
  labs(x="# of Days Ago", y = NULL) +
  theme(legend.position="bottom", panel.background = element_blank()) +
  guides(colour = guide_legend("No beacon tag", override.aes = list(colour = "black")))

```


<br/>


## Voltage for each ATS receiver

<br/>

```{r print voltage for ATS receivers,fig.height = 30, echo=FALSE, warning=FALSE, message=FALSE}

library(ggplot2)

volt.files <- data.frame(path =dir(pattern = "*.txt", path = "C:/Users/Trailer/Documents/Voltage_emails", full.names = T), stringsAsFactors = F)
volt.files$Datetime <- as.POSIXct(substr(volt.files$path, 43, stop = 57), format = "%Y%m%d-%H%M%S")

## Only use last 2 weeks of volt files
volt.files_2weeks <- volt.files[volt.files$Datetime > (Sys.time() - 60*60*24*14),]

IPs <- read.csv("C:/Users/Trailer/Desktop/Real-time data massaging/products/IP_addresses.csv", stringsAsFactors = F)
volt.files_2weeks$IP <- NULL
volt.files_2weeks$Volts <- NULL

for (i in 1:nrow(volt.files_2weeks)){
  test <- read.csv(volt.files_2weeks[i,"path"], skip = 3, stringsAsFactors = F)
  volt.files_2weeks[i,"Volts"] <- as.numeric(substr(test[1,],11,15))
  volt.files_2weeks[i,"IP"] <- substr(test[2,],5,nchar(test[2,]))
}

volt.files_2weeks <- merge(volt.files_2weeks, IPs[,c("RECV", "TCP_IP")], by.x = "IP", by.y = "TCP_IP")

volt.files_2weeks <- volt.files_2weeks[,c("Datetime", "Volts", "RECV")]

## Get last 2 weeks of Teknos
volt.teknos <- downloads[downloads$recv %in% c(157002,157003,157004,157005) & downloads$end > (Sys.time() - 60*60*24*14), c("end", "bat_volts", "recv") ]

colnames(volt.teknos) <- colnames(volt.files_2weeks)

volt.files_2weeks <- rbind(volt.files_2weeks, volt.teknos)
volt.files_2weeks$Volts <- as.numeric(volt.files_2weeks$Volts)

ggplot(volt.files_2weeks, aes(x = Datetime, y = Volts)) + 
  geom_line() + 
  geom_point() +
  ylim(9,15) +
  scale_x_datetime(date_breaks = "2 days", date_labels = "%Y-%m-%d") +
  geom_hline(yintercept=12, linetype="dashed", color = "red") +
  facet_grid(RECV ~ .) + 
  theme(legend.position = "none")

```


<br/>

```{r see winter run counts per tagger, echo=FALSE}

# setwd(paste(file.path(Sys.getenv("USERPROFILE"),"Desktop",fsep="\\"), "\\Real-time data massaging\\products", sep = ""))
# 
# 
# detects <- read.csv("detects_Winter_H_2018.csv", stringsAsFactors = FALSE)
# tagging <- read.csv("Tagged_Fish_Metadata.txt", stringsAsFactors = F)
# tagging <- tagging[tagging$StudyID == "Winter_H_2018",]
# 
# detects_tagger <- merge(detects, tagging[,c("TagID_Hex","Surgeon", "Rel_group")], by.x = "TagCode", by.y = "TagID_Hex")
# 
# arrivals <- as.data.frame(table(detects_tagger[!duplicated(detects_tagger$TagCode),"Surgeon"]))
# 
# colnames(arrivals) <- c("Surgeon", "Unique_Fish_Arrivals")
# arrivals$total_tagged <- 200
# arrivals$percent_arrivals <- round(arrivals$Unique_Fish_Arrivals/arrivals$total_tagged * 100,2)
# 
# kable(arrivals)


```


